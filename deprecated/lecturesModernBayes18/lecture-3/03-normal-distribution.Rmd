
---
title: "Module 3: Introduction to the Normal Distribution"
author: "Rebecca C. Steorts"
output: 
     beamer_presentation:
      includes: 
          in_header: custom2.tex
font-size: 8px
---
Announcements
===
- Exam I: Feb 8, in class
- Exam I: closed notes
- Exam I: will answer questions regarding what is on the exam on Thursday. Review session will be on Tuesday Feb 6. 
- Homework 2 due today 10 PM, Sakai.
- Homework 3 posted, due Sunday 10 PM. Based on lab from tomorrow. 
- Exam I material: everything we have covered in class, hw, lab so far (including today and lab tomorrow). 
- See Sakai for exam cover page. 

Exercise
===
Suppose $a < x < b.$ Consider the notation
$I_{(a,b)}(x),$ where $I$ denotes the indicator function. We define $I_{(a,b)}(x)$ to be the following:
$$
I_{(a,b)}(x)=
\begin{cases} 
1 & \text{if $a < x < b$,}
\\
0 &\text{otherwise.}
\end{cases}
$$

Let 
\begin{align*}
X|\theta &\sim \text{Uniform}(0,\theta)\\
\theta &\sim \text{Pareto}(\alpha,\beta),
\end{align*}
where $p(\theta) = \dfrac{\alpha \beta^\alpha}{\theta^{\alpha +1}}I_{(\beta,\infty)}(\theta).$
Write out the likelihood $p(x\mid \theta).$ Then calculate the posterior distribution of $\theta|x.$  

Solution
===
\begin{align}
P(\theta \mid x)
&\propto p(x \mid \theta) p(\theta)\\
& \propto \frac{1}{\theta} I(0 < x < \theta)
\frac{\alpha \beta^{\alpha}}{\theta^{\alpha+1}}I(\theta > \beta) \\
& \propto \frac{\alpha \beta^{\alpha}}
{\theta^{\alpha+2}}I(\theta> x) I(\theta > \beta) \\
& \propto 
\frac{\alpha \beta^{\alpha}}
{\theta^{\alpha+2}}
I( \theta > \max\{x, \beta \}).
\end{align}

Thus, $$\theta \mid x \sim \text{Pareto} (\alpha + 1, \max\{x, \beta \}).$$

Agenda
===
- The normal distribution
- Common properties
- The normal uniform
- The normal-normal model

Normal distribution
===

The normal distribution $\N(\mu,\sigma^2)$

- with mean $\mu\in\R$ and variance $\sigma^2 > 0$ - (standard deviation $\sigma =\sqrt{\sigma^2}$) has p.d.f.
$$\N(x\mid\mu,\sigma^2) =\frac{1}{\sqrt{2\pi\sigma^2}}\exp\Big(-\frac{1}{2\sigma^2}(x-\mu)^2\Big) $$
for $x\in\R$. 

It is often more convenient to write the p.d.f.\ in terms of the **precision**, or inverse variance, $\lambda = 1/\sigma^2$ rather than the variance. 

Re-parameterized Normal
===
In this parametrization, the p.d.f.\ is
$$\N(x\mid\mu,\lambda^{-1}) =\sqrt{\frac{\lambda}{2\pi}}\exp\big(-\tfrac{1}{2}\lambda (x-\mu)^2\big) $$
since $\sigma^2 = 1/\lambda =\lambda^{-1}$.


\begin{figure}
  \begin{center}
    \includegraphics[width=1\textwidth]{examples/normal.png}
    % Source: Original work by J. W. Miller.
  \end{center}
  \caption{Normal distribution with various choices of $\mu$ and $\sigma$.}
\end{figure}

Normality?
===

- The central limit theorem (CLT) states that the sum of a large number of independent random variables tends to be approximately normally distributed.  
- Real world data often appears approximately normal. 

Normality?
===

- Human heights and other body measurements, 
- Cumulative hydrologic measures such as annual rainfall or monthly river discharge, 
- Errors in astronomical or physical observations, 
- Diffusion of a substance in a liquid or gas. 

- Some things are products of many independent variables (rather than sums), and in such cases the logarithm will be approximately normal since it is a sum of many independent variables

Example: stock market indices, due to the effect of compound interest.

Properties of the Normal distribution
===
\begin{itemize}
\item Mean, median, and mode are all the same ($\mu$)
\item Symmetric about the mean
\item 95\% probability within $\pm 1.96\sigma$ of the mean (roughly, $\pm 2\sigma$)
\item If $X\sim\N(\mu,\sigma^2)$ and $Y\sim\N(m,s^2)$ independently, then
\begin{align}\label{equation:linear-combination}
a X + b Y\sim\N(a\mu + b m,\,a^2\sigma^2+ b^2 s^2).
\end{align}
\item Careful: \texttt{rnorm}, \texttt{dnorm}, \texttt{pnorm}, and \texttt{qnorm} in \texttt{R} take the mean and \textcolor{red}{standard deviation} $\sigma$ as arguments (not mean and variance $\sigma^2$). For example, \texttt{rnorm(n,m,s)} generates $n$ normal random variables from $\N(m,s^2)$.
\end{itemize}

Normal-Uniform
===
$$ X_1,\dotsc,X_n \mid \theta \iid\N(\theta,\sigma^2). $$

Assume the prior on $\theta$ is constant over the real line. We can write this as $p(\theta) \propto 1.$

Derive the posterior distribution. 

Solution
===
\begin{align}
p(\theta \mid x_{1:n}) 
&\propto
\N(\theta,\sigma^2) \times 1 \\
& \propto (\frac{\ell}{2\pi})^{n/2}\exp\big(-\tfrac{1}{2}\ell \sum_i(x_i-\theta)^2\big)\notag\\
& \propto \exp\big(-\tfrac{1}{2}\ell \sum_i(x_i -\bar{x} + \bar{x} - \theta)^2\big)\notag\\
& \propto \exp\big(-\tfrac{1}{2}\ell \sum_i(x_i -\bar{x})^2)
\exp\big(-\tfrac{1}{2}\ell \sum_i( \bar{x} - \theta)^2\big)\notag\\
&\propto 
\exp\big(-\tfrac{1}{2}\ell \sum_i( \bar{x} - \theta)^2\big) \\
&= \exp\big(-\tfrac{n \ell}{2} \sum_i(\theta - \bar{x} )^2\big)
\end{align}

This implies that 
$$\theta \mid x_{1:n} \sim N(\bar{x}, (n\ell)^{-1})$$

Normal-Normal
===
$$ X_1,\dotsc,X_n \mid \theta \iid\N(\theta,\lambda^{-1}). $$
Assume the precision $\lambda = 1/\sigma^2$ is known and fixed, and $\theta$ is given a $\N(\mu_0,\lambda_0^{-1})$ prior:
$$\btheta \sim \N(\mu_0,\lambda_0^{-1})$$
i.e., $p(\theta) = \N(\theta\mid \mu_0,\lambda_0^{-1})$. This is sometimes referred to as a **Normal--Normal** model.

Posterior derivation
===
We begin with the **likelihood** of the normal distribution. 

\textcolor{red}{For any $x$ and $\ell$},
\begin{align}\label{equation:prop}
\N(x\mid\theta,\ell^{-1})
& =\sqrt{\frac{\ell}{2\pi}}\exp\big(-\tfrac{1}{2}\ell (x-\theta)^2\big)\notag\\
&\underset{\theta}{\propto} \exp\big(-\tfrac{1}{2}\ell (x^2 - 2 x \theta +\theta^2)\big)\notag\\
&\textcolor{red}{\underset{\theta}{\propto} \exp\big(\ell x \theta -\tfrac{1}{2}\ell\theta^2)\big)}.
\end{align}

Note: we drop the **constant term** and we will do this often when working with the normal distribution. 


Posterior derivation (continued)
===
We now consider the **prior** distribution on $\theta.$

Due to the symmetry of the normal p.d.f.,
\begin{align}\label{equation:prior}
\N(\theta\mid\mu_0,\lambda_0^{-1}) =\N(\mu_0\mid\theta,\lambda_0^{-1})
\underset{\theta}{\propto}\exp\big(\lambda_0\mu_0\theta-\tfrac{1}{2}\lambda_0\theta^2\big),
\end{align}
where $x=\mu_0$ and $\ell=\lambda_0$.

Posterior derivation (continued)
===
Let $$L =\lambda_0+ n\lambda \quad \text{and} \quad
M =\frac{\lambda_0\mu_0+\lambda\sum_{i = 1}^n x_i}{\lambda_0+ n\lambda}.$$
\begin{align*}
p(\theta|x_{1:n})&\propto p(\theta) p(x_{1:n}|\theta) \\
&= \N(\theta\mid\mu_0,\lambda_0^{-1})\prod_{i = 1}^n\N(x_i\mid \theta,\lambda^{-1})\\
&\overset{\text{(a)}}{\propto} \exp\big(\lambda_0\mu_0\theta-\tfrac{1}{2}\lambda_0\theta^2\big)
         \exp\big(\lambda (\textstyle\sum x_i) \theta -\tfrac{1}{2}n\lambda\theta^2\big)\\
&= \exp\Big((\lambda_0\mu_0+\lambda\textstyle\sum x_i)\theta-\tfrac{1}{2}(\lambda_0+ n\lambda)\theta^2\Big)\\
&= \exp(L M\theta-\tfrac{1}{2}L\theta^2)\\
&\overset{\text{(b)}}{\propto} \N(M\mid\theta,L^{-1}) =\N(\theta\mid M,L^{-1}),
\end{align*}
where step (a) uses Equations \ref{equation:prop} and \ref{equation:prior}, and step (b) uses Equation \ref{equation:prop} with $x=M$ and $\ell=L$.


Posterior derivation (continued)
===
Recall $$L =\lambda_0+ n\lambda \quad \text{and} \quad
M =\frac{\lambda_0\mu_0+\lambda\sum_{i = 1}^n x_i}{\lambda_0+ n\lambda}.$$

It turns out that the posterior is
$$
\begin{aligned}\label{equation:posterior}
\btheta|x_{1:n}\, \sim \,\N(M,L^{-1})
\end{aligned}
$$

i.e., $p(\theta|x_{1:n}) =\N(\theta\mid M,L^{-1})$.


Thus, the normal distribution is, itself, a conjugate prior for the mean of a normal distribution with known precision.

Heights of Adult Humans
===
- Heights tend to be normally distributed because there are many independent genetic and environmental factors which contribute additively to overall height
- This leads to a normal distribution due to the central limit theorem. 
\begin{figure}
  \begin{center}
    \includegraphics[width=1\textwidth]{examples/heights-female-male.png}
    % Source: Original work by J. W. Miller.
  \end{center}
  \caption{Estimated densities of the heights of Dutch women and Dutch men based on a sample of 695 women and 562 men.}
  \label{figure:heights}
\end{figure}


Heights of Adult Humans
===
- Consider combined distribution of heights (pooling females and males together). Would this be normal? 
- It is thought that such data is bimodal (having two maxima). Is it really bimodal? (See, Schilling et al.\ (2002))
\begin{figure}
  \begin{center}
    \includegraphics[width=1\textwidth]{examples/heights-combined.png}
    % Source: Original work by J. W. Miller.
  \end{center}
  \caption{Estimated density for Dutch women and men together, assuming there is an equal proportion of women and men in the population.}
  \label{figure:heights-combined}
\end{figure}



Heights of Adult Humans, Combined
===
At a glance, while the heights of women and men separately do appear to be roughly normally distributed, the combined distribution does not look bimodal.  How could we test whether it is bimodal in a more precise way?

Our Assumptions
===
- Assume female heights and male heights are each normally distributed. 
- Let's assume they have the same standard deviation, and also that there is an equal proportion of women and men in the population. 
- Then, it is known that the combined distribution is bimodal if and only if the difference between the means is greater than twice the standard deviation (Helguerro, 1904).

Model
===
In mathematical notation: Assume the female heights are
$$X_1,\dotsc,X_k\iid\N(\theta_f,\sigma^2),$$
where $k=695$, the male heights are
$$Y_1,\dotsc,Y_\ell\iid\N(\theta_m,\sigma^2),$$
where $\ell=562$, and the p.d.f.\ of the combined distribution of heights is
$$\tfrac{1}{2}\N(x\mid\theta_f,\sigma^2)+\tfrac{1}{2}\N(x\mid\theta_m,\sigma^2). $$
(This is an example of what is called a two-component \term{mixture} distribution.) 

Model
===
Let's put independent normal priors on $\theta_f$ and $\theta_m$:
$$ p(\theta_f,\theta_m) = p(\theta_f) p(\theta_m) 
=\N(\theta_f\mid \mu_{0,f},\sigma_0^2)\N(\theta_m\mid \mu_{0,m},\sigma_0^2).$$

- Assume $\sigma^2$ is known. 
- For the purposes of this example, let's use $\sigma=8$ centimeters (about 3 inches). 
- Based on common knowledge of typical human heights, let's choose the prior parameters (a.k.a. hyperparameters) as follows:
\begin{center}
\begin{tabular}{cll}
$\mu_{0,f}$ & (mean of prior on female mean ht) & \text{165 cm ($\approx$ 5 ft, 5 in)}\\
$\mu_{0,m}$ & (mean of prior on male mean ht) & \text{178 cm ($\approx$ 5 ft, 10 in)}\\
$\sigma_0$ & (std.\ dev.\ of priors on mean ht) & \text{15 cm ($\approx$ 6 in)}\\
\end{tabular}
\end{center}

Bimodal Fact
===
It is known (Helguerro, 1904) that the combined distribution is bimodal if and only if
$$ |\theta_f-\theta_m|>2\sigma. $$
So, to address our question of interest (``Is human height bimodal?''), we would like to compute the posterior probability that this is the case, i.e., we want to know
$$ \Pr(\text{bimodal}\mid \text{data}) = \Pr\big(|\btheta_f-\btheta_m|>2\sigma \mid x_{1:k},y_{1:\ell}\big).$$ 

Results
===
\begin{figure}
  \begin{center}
    \includegraphics[width=1\textwidth]{examples/heights-prior-posterior.png}
    % Source: Original work by J. W. Miller.
  \end{center}
  \caption{Priors and posteriors for the mean heights of Dutch women and men.}
  \label{figure:heights-prior-posterior}
\end{figure}

Results (continued)
===
We can compute the posteriors for $\theta_f$ and $\theta_m$ using Equation \ref{equation:posterior} for each of them, independently.  Figure \ref{figure:heights-prior-posterior} shows the priors and posteriors. 
\begin{itemize}
\item Sample means: $\bar x = 168.0$ cm (5 feet 6.1 inches) for females, and $\bar y = 181.4$ cm (5 feet 11.4 inches) for males.
\item Posterior means: $M_f = 168.0$ cm for females, and $M_m = 181.4$ cm for males. (Essentially identical to the sample means, due to the relatively large sample size and relatively weak prior.)
\item Posterior standard deviations: $1/\sqrt{L_f} = 0.30$ cm and $1/\sqrt{L_m} = 0.34$ cm.
\end{itemize}



Results (continued)
===
By Equation \ref{equation:linear-combination} (a linear combination of independent normals is normal),
$$\btheta_m-\btheta_f\mid x_{1:k},y_{1:\ell}\,\sim\,\N(M_m-M_f,\, L_m^{-1} + L_f^{-1}) = \N(13.4,0.45^2) $$
so we can compute $\Pr(\text{bimodal}\mid \text{data})$ using the normal c.d.f.\ $\Phi$:
\begin{align*}
&\Pr(\text{bimodal}\mid \text{data})= \Pr\big(|\btheta_m-\btheta_f|>2\sigma \mid x_{1:k},y_{1:\ell}\big) \\
&= \Phi(-2\sigma\mid 13.4,0.45^2)
+ \big(1-\Phi(2\sigma\mid 13.4,0.45^2)\big)\\
& = 6.1\times 10^{-9}.
\end{align*}


Intuitive interpretation: The posteriors are about 13 or 14 centimeters apart, which is under the $2\sigma = 16$ threshold for bimodality, and they are sufficiently concentrated that the posterior probability of bimodality is essentially zero.


Exercise
===
Suppose $a < x < b.$ Consider the notation
$I_{(a,b)}(x),$ where $I$ denotes the indicator function. We define $I_{(a,b)}(x)$ to be the following:
$$
I_{(a,b)}(x)=
\begin{cases} 
1 & \text{if $a < x < b$,}
\\
0 &\text{otherwise.}
\end{cases}
$$

Let 
\begin{align*}
X_1 \ldots, X_n|\theta &\sim \text{Uniform}(0,\theta)\\
\theta &\sim \text{Pareto}(\alpha,\beta),
\end{align*}
where $p(\theta) = \dfrac{\alpha \beta^\alpha}{\theta^{\alpha +1}}I_{(\beta,\infty)}(\theta).$

Write out the likelihood $p(x\mid \theta).$ Then calculate the posterior distribution of $\theta|x.$  

Likelihood
===

\begin{align}
p(x_{1:n} \mid \theta) &= \prod_{i=1}^n \frac{1}{\theta} I(\theta > x_i)\\
&= \frac{1}{\theta^n} \prod_{i=1}^n I(\theta > x_i)\\
&= \frac{1}{\theta^n} I( \theta > \max \{ x_{1:n} \})
\end{align}


Posterior 
===

\begin{align}
p(\theta \mid x_{1:n}) &\propto
\frac{1}{\theta^n} I( \theta > \max \{ x_{1:n} \}) \frac{\alpha \beta^{\alpha}}{\theta^{\alpha+1}}I(\theta > \beta)\\
& = \frac{\alpha \beta^{\alpha}}{\theta^{n+\alpha+1}}I(\theta > \beta)I( \theta > \max \{ x_{1:n} \})\\
& = \frac{\alpha \beta^{\alpha}}{\theta^{n+\alpha+1}}
I( \theta > \max\{\beta, \max \{ x_{1:n} \}\}) \\
& = \frac{\alpha \beta^{\alpha}}{\theta^{n+\alpha+1}}
I( \theta > \max\{\beta,  x_{1:n} \}) 
\end{align}

Exam I
===
- Closed notes, closed book.
- You will be given cover page with distributions (see Sakai under Annoucements).
- Please come on time. 
- Material covers lectures (slides and written material in class), labs, and homeworks. 
- Material covers: Modules 0--3. 

Exam I
===
- To help you study, work through the practice problems
from class and the ones at the end of homework 3. (Solutions on Sakai). 
- 5 problems total. 

Topics to Review
===
- Bayes Theorem (derivation)
- Derivation of posterior distributions, marginals
- How do you show the proprietry of the posterior distribution?
- Conjugate families (i.e Beta-Bernoulli, Beta-Binomial, Galenshore, etc.)
- For extra problems, please see "Some of Bayesian Methods", Chapter 1--2. All examples are worked in the text. 

Topics to Review (continued)
===
- What is a loss function (examples)
- What is the Bayes procedure? Know how to derive it.
- Posterior risk, frequentist risk. Derivations, understand plots, know the differences of when to use these.
- Review lab concepts. Suppose I describe some data to you. What would distribution might characterize it well. What prior would be appropriate? 

Topics that will not be covered on exam I
===
- pseudocode, R code
- integrated risk
- admissibility


