
---
title: "Module 8: Part III: Gibbs Sampling and Data Augmentation"
author: "Rebecca C. Steorts"
output: 
     beamer_presentation:
      includes: 
          in_header: custom2.tex
font-size: 8px
---

Agenda
===
- Data Augmentation
- Dutch Example
- Other example


Data augmentation for auxiliary variables
===
\begin{itemize}
\item A commonly-used technique for designing MCMC samplers is to use \emph{data augmentation}, also known as \emph{auxiliary variables}.
\item Introduce variable(s) $Z$ that depends on the distribution of the existing variables in such a way that the
resulting conditional distributions, with $Z$ included, are easier to sample from and/or result in better mixing.
\item  $Z$'s are latent/hidden variables that are introduced for the purpose of simplifying/improving the sampler.
\end{itemize}

Idea: Create Z's and throw them away at the end! 
===
\begin{itemize}
\item Suppose we want to sample from $p(x,y)$, but $p(x|y)$ and/or $p(y|x)$ are complicated. 
\item Choose $$p(z|x,y)$$ such
that $p(x|y,z)$, $p(y|x,z)$, and $p(z|x,y)$ are easy to sample from. 
\item Then construct a Gibbs sampler to sample all three variables
$(X,Y,Z)$ from $p(x,y,z)$.
\item Then we just throw away the $Z$'s and we will have samples $(X,Y)$ from $p(x,y)$.
\end{itemize}

Dutch Example
===
Consider a data set on the heights of 695 Dutch women and 562 Dutch men.  

\vskip 1em

Suppose we have the list of heights, but we don't know which data points
are from women and which are from men.  

Dutch Example 
===
From Figure \ref{figure:heights-combined} can we still infer the distribution of female heights and male heights?

\begin{figure}
  \begin{center}
    \includegraphics[width=1\textwidth]{examples/heights-combined}
    % Source: Original work by J. W. Miller.
  \end{center}
  \caption{Heights of Dutch women and men, combined.}
  \label{figure:heights-combined}
\end{figure}
Surprisingly, the answer is yes!

Dutch example
===
What's the magic trick?
\vskip 1em

The reason is that this is a two-component mixture of Normals,
and there is an (essentially) unique set of mixture parameters corresponding to any such distribution.

\vskip 1em

We'll get to details soon. Be patient!

Constructing a Gibbs sampler
===
To construct a Gibbs sampler for this situation:

\begin{itemize}
\item Common to introduce an auxiliary variable $Z_i$ for each data point,
indicating which mixture component it is drawn from.
\item  In this example, $Z_i$ indicates whether subject $i$ is female or male.
\item This results in a Gibbs sampler that is easy to derive/implement.
\end{itemize}

Two component mixture model
===
Let's assume that both mixture components (female and male) have the same precision, say $\lambda$, and that $\lambda$ is fixed and known.  

\vskip 1em

Then the usual two-component Normal mixture model is:
\begin{align}
  & X_1,\ldots, X_n \mid \mu,\pi\ \sim F(\mu,\pi)\\
      & \mu:=(\mu_0,\mu_1) \stackrel{iid}{\sim} \N(m,\ell^{-1})\\
        & \pi \sim \Beta(a,b)
\end{align}

where $F(\mu,\pi)$ is the distribution with p.d.f. 
$$ f(x|\mu,\pi) = (1-\pi)\N(x\mid \mu_0,\lambda^{-1}) + \pi
\N(x\mid \mu_1,\lambda^{-1}) $$
and $\mu=(\mu_0,\mu_1)$.


Likelihood
===
The likelihood is
\begin{align*}
    p(x_{1:n}|\mu,\pi) &= \prod_{i=1}^n f(x_i|\mu,\pi) \\
                       & = \prod_{i=1}^n \Big[ (1-\pi)\N(x_i\mid \mu_0,\lambda^{-1}) + \pi\N(x_i\mid \mu_1,\lambda^{-1}) \Big] 
\end{align*}
which is a complicated function of $\mu$ and $\pi$, making the posterior difficult to sample from directly.

Latent allocation variables to the rescue!
===
Define an equivalent model that includes latent ``allocation'' variables $Z_1,\ldots,Z_n$
\vskip 1em

These indicate which mixture component each data point
comes from--that is, $Z_i$ indicates whether subject $i$ is female or male.
\begin{align}
 & X_i\sim\N(\mu_{Z_i},\lambda^{-1}) \text{ independently for } i=1,\ldots,n.\\
 & Z_1,\ldots,Z_n \mid \mu,\pi\,\stackrel{iid}{\sim}\,\Bernoulli(\pi)\\
  & \mu= (\mu_0,\mu_1) \stackrel{iid}{\sim}\N(m,\ell^{-1})\\
    & \pi \sim \Beta(a,b)
\end{align}

Latent allocation variables
===
Recall
\begin{align*}
 & X_i\sim\N(\mu_{Z_i},\lambda^{-1}) \text{ independently for } i=1,\ldots,n.\\
     & Z_1,\ldots,Z_n|\mu,\pi\,\stackrel{iid}{\sim}\,\Bernoulli(\pi)\\
    & \mu= (\mu_0,\mu_1) \stackrel{iid}{\sim} \N(m,\ell^{-1})\\
    & \pi \sim \Beta(a,b)
\end{align*}
This is equivalent to the model above, since
\begin{align}
    &p(x_i|\mu,\pi) \\
    &= p(x|Z_i=0,\mu,\pi)\Pr(Z_i=0|\mu,\pi) 
                    + p(x|Z_i=1,\mu,\pi)\Pr(Z_i=1|\mu,\pi) \\
            &= (1-\pi)\N(x_i|\mu_0,\lambda^{-1}) + \pi\N(x_i|\mu_1,\lambda^{-1})\\
            &= f(x_i|\mu,\pi),
\end{align}
and thus it induces the same distribution on $(x_{1:n},\mu,\pi)$. The latent model is considerably easier to work with, particularly for Gibbs
sampling. 

Full conditionals
===
\begin{itemize}
    \item ($\pi|\cdots$) Given $z$, $\pi$ is independent of everything else, so this reduces to a Beta--Bernoulli model, and we have
        $$ p(\pi|\mu,z,x) = p(\pi|z) =\Beta(\pi\mid a + n_1,\, b + n_0) $$
        where $n_k = \sum_{i=1}^n \I(z_i=k)$ for $k \in \{0, 1\}$.
        \end{itemize}
        
Full conditionals
===
\begin{itemize}        
    \item ($\mu|\cdots$) Given $z$, we know which component each data point comes from. 
    \vskip 1em
    The model (conditionally on $z$) is just two
        independent Normal--Normal models, as we have seen before:
        \begin{align*}
            \bm\mu_0|\mu_1,x,z,\pi\, \sim \,\N(M_0,L_0^{-1})\\
            \bm\mu_1|\mu_0,x,z,\pi\, \sim \,\N(M_1,L_1^{-1})
        \end{align*}
        where for $k\in\{0,1\}$,
        \begin{align*}
        n_k &= \sum_{i=1}^n \I(z_i=k)\\
        L_k &=\ell + n_k\lambda\\
        M_k &=\frac{\ell m + \lambda\sum_{i:z_i=k} x_i}{\ell + n_k\lambda}.
        \end{align*}
        \end{itemize}
        
Full conditionals
===
\begin{itemize}
    \item ($z|\cdots$)
        \begin{align*}
            p(z|\mu,\pi,x)&\underset{z}{\propto}p(x,z,\pi,\mu)\underset{z}{\propto} p(x|z,\mu) p(z|\pi)\\
            & =\prod_{i = 1}^n \N(x_i|\mu_{z_i},\lambda^{-1})\Bernoulli(z_i|\pi)\\
            & =\prod_{i = 1}^n \Big(\pi\N(x_i|\mu_1,\lambda^{-1})\Big)^{z_i} \Big((1-\pi)\N(x_i|\mu_0,\lambda^{-1})\Big)^{1-z_i}\\
            & =\prod_{i = 1}^n \alpha_{i,1}^{z_i} \alpha_{i,0}^{1-z_i}\\
            &\underset{z}{\propto}\prod_{i = 1}^n\Bernoulli(z_i\mid \alpha_{i,1}/(\alpha_{i,0}+\alpha_{i,1}))
        \end{align*}
        where
        \begin{align*}
            \alpha_{i,0} & =(1-\pi)\N(x_i|\mu_0,\lambda^{-1})\\
            \alpha_{i,1} & =\pi\N(x_i|\mu_1,\lambda^{-1}).
        \end{align*}
\end{itemize}



My Factory Settings!
===
\begin{itemize}
\item $\lambda = 1/\sigma^2$ where $\sigma = 8$ cm ($\approx 3.1$ inches) ($\sigma$ = standard deviation of the subject heights within each component)
\item $a = 1$, $b = 1$ (Beta parameters, equivalent to prior ``sample size'' of 1 for each component)
\item $m = 175$ cm ($\approx 68.9$ inches) (mean of the prior on the component means)
\item $\ell = 1/s^2$ where $s = 15$ cm ($\approx 6$ inches) ($s$ = standard deviation of the prior on the component means)
\end{itemize}

My Factory Settings!
===
We initialize the sampler at:
\begin{itemize}
\item $\pi = 1/2$ (equal probability for each component)
\item $z_1,\ldots,z_n$ sampled i.i.d.\ from $\Bernoulli(1/2)$ (initial assignment to components chosen uniformly at random)
\item $\mu_0 =\mu_1 = m$ (component means initialized to the mean of their prior)
\end{itemize}


Results
===
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.9\textwidth]{examples/mix-mu_trace-a.png}\caption{Traceplots of the component means, $\mu_0$ and $\mu_1$.}
\label{default}
\end{center}
\end{figure}


Results
===
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.9\textwidth]{examples/mix-histograms_at_last_sample-a.png}\caption{Histograms of the heights of subjects assigned to each component, according to $z_1,\ldots,z_n$, in a typical sample.}
\label{default}
\end{center}
\end{figure}

    
Results from two runs of the mixture model
===    
\begin{figure}
\begin{center}
\includegraphics[width=0.55\textwidth]{examples/mix-p_trace-a.png}
\includegraphics[width=0.55\textwidth]{examples/mix-p_trace-b}
\caption{Traceplots of the mixture weight, $\pi$.}
\label{figure:mixb}
\end{center}  
\end{figure}

Caution: watch out for modes
===
Example illustrates a big thing that can go wrong with MCMC (although fortunately, in this case, the results are still valid if interpreted correctly). 
\begin{itemize}
\item Why are females assigned to component 0 and males assigned to component 1? Why not the other way around? 
\item In fact, the model is symmetric with respect to the two components, and thus the posterior is also symmetric. 
\item If we run the sampler multiple times (starting from the same initial values), sometimes it will settle on females as 0 and males as 1, and sometimes on females as 1 and males as 0 --- see Figure \ref{figure:mixb}. 
\item Roughly speaking, the posterior has two modes.
\item  If the sampler were behaving properly, it would move back and forth between these two modes.
\item But it doesn't---it gets stuck in one and stays there.
\end{itemize}

Takeaway from example
===
- This is a very common problem with mixture models. 
- Fortunately, however, in the case of mixture models, the results are still valid if we interpret them correctly. 
- Specifically, our inferences will be valid as long as we only consider quantities that are invariant with respect to permutations of the components (e.g. symmetry about the mean).

Three component mixture model
===
- Consider a three component mixture of normal distribution with a common prior on the mixture component means, the error variance and the variance within mixture component means. 
- The prior on the mixture weights $w$ is a three component Dirichlet distribution. 

\begin{align*}
p(Y_i | \mu_1,\mu_2,\mu_3,w_1,w_2,w_3, \varepsilon^2) 
&= \sum_{j=1}^3 w_i N(\mu_j, \varepsilon^2)\\
\mu_j|\mu_0,\sigma_0^2 &\sim N(\mu_0,\sigma_0^2)\\
\mu_0 &\sim N(0,3)\\
\sigma_0^2 &\sim \text{InverseGamma}(2,2)\\
(w_1,w_2,w_3) &\sim \text{Dirichlet}(1,1,1)\\
\varepsilon^2 
&\sim \text{InverseGamma}(2,2),
\end{align*}
for $i=1,\ldots n.$

Three component mixture model
===
Specifically, 
\begin{itemize}
\item $w_1,w_2$ and $w_3$ are the mixture weight of mixture components 1,2 and 3 respectively
\item $\mu_1,\mu_2$ and $\mu_3$ are the means of the mixture components 
\item $\varepsilon^2$ is the variance parameter of the error term around the mixture components.
\end{itemize}

Dirichlet
===
A Dirichlet distribution is a distribution of the $K$-dimensional probability simplex
$$\bigtriangleup_K = \{(\pi_1,\ldots, \pi_k): \pi_k \geq 0, \sum_k \pi_k = 1\}$$

We say that $(\pi_1,\ldots, \pi_k)$ is Dirichlet distributed:

$$(\pi_1,\ldots, \pi_k)\sim \text{Dir}(\alpha_1,\ldots,\alpha_k)$$
if
$$p(\pi_1,\ldots, \pi_k) = \frac{\Gamma(\sum_k \alpha_k)}
{\prod_k \Gamma(\alpha_k)} 
\prod_{k=1}^K \pi_k^{\alpha_{k-1}}$$

Dirichlet distribution
===
Let
$$\theta \sim \text{Dirichlet}(\alpha_1,\ldots,\alpha_k)$$
where the probability density function is

$$p(\theta \mid \alpha) \propto \prod_{k=1}^m \theta_k^{\alpha_k -1},$$ where
$\sum_k \theta_k =1, \theta_i \geq 0$ for all i

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.5\textwidth]{figures/simplex}
\caption{3 dimensional support of the $\theta$ space. Called the simplex!}
\end{center}
\end{figure}

Dirichlet distribution
===
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.9\textwidth]{figures/dir}
\caption{Far left: We get a uniform prior on the simplex. Moving to the right we get things unimodal. On the bottom, we get distributions that are multimodal at the corners.}
\label{default}
\end{center}
\end{figure}

Three component mixture model (Task 1 and 2)
===
Derive the full conditionals for all the parameters up to a normalizing constant.


Task 1 and 2
===
Specifically, you should derive the following conditional distributions below:

\begin{itemize}
\item $p(w_1,w_2,w_3|\mu_1,\mu_2,\mu_3,\varepsilon^2,Y_1,...,Y_N) \propto$
\item $p(\mu_1|\mu_2,\mu_3,w_1,w_2,w_3,Y_1,...,Y_N,\varepsilon^2,\mu_0,\sigma_0^2) \propto$
\item $p(\mu_2|\mu_1,\mu_3,w_1,w_2,w_3,Y_1,...,Y_N,\varepsilon^2,\mu_0,\sigma_0^2) \propto$
\item $p(\mu_3|\mu_1,\mu_2,w_1,w_2,w_3,Y_1,...,Y_N,\varepsilon^2,\mu_0,\sigma_0^2) \propto$
\item $p(\varepsilon^2|\mu_1,\mu_2,\mu_3,Y_1,...,Y_N) \propto$
\item $p(\mu_0|\mu_1,\mu_2,\mu_3,\sigma_0^2) \propto$
\item $p(\sigma_0^2|\mu_0,\mu_1,\mu_2,\mu_3) \propto$
\end{itemize}

Task 1 (Solution)
===
We start by deriving the full conditional kernels. 
\begin{align}
p(\mu_0|\mu_1,\mu_2,\mu_3,\varepsilon^2,\sigma_0^2) & \propto \text{Normal-Normal mean update}\\
p(\sigma_0^2|\mu_1,\mu_2,\mu_3,\mu_0) & \propto \text{Normal-InverseGamma variance update}
\end{align}

Task 1 (Solution)
===
\begin{align}
&p(\mu_k|Y_1,...,Y_N,\sigma_0^2,\varepsilon^2,w_1,w_2,w_3) \\
& \propto  \frac{1}{\sqrt{2 \pi \sigma_0^2}}e^{-\frac{1}{\sigma_0^2}(\mu_k -\mu_0)^2}\prod_{i=1}^N \Big(\sum_{j=1}^3 w_j\frac{1}{\sqrt{2\pi \varepsilon^2}}e^{-\frac{1}{2\varepsilon^2}(Y_i - \mu_j)^2}\Big)\\
&\propto ? 
\end{align}

Task 1 (Solution)
===
\begin{align}
& p(\varepsilon^2|Y_1,...,Y_N,\mu_1,\mu_2,\mu_3,w_1,w_2,w_3)\\ & \propto(\varepsilon^2)^{-3}e^{-\frac{2}{\varepsilon^2}} \prod_{i=1}^N \Big(\sum_{j=1}^3 w_j\frac{1}{\sqrt{2\pi \varepsilon^2}}e^{-\frac{1}{2\varepsilon^2}(Y_i - \mu_j)^2}\Big)\\
&\propto ? 
\end{align}

Task 1 (Solution)
===
\begin{align}
&p(w_1,w_2,w_3|Y_1,...,Y_N,\mu_1,\mu_2,\mu_3,\varepsilon^2) \\
&\propto \prod_{i=1}^N \Big(\sum_{j=1}^3 w_j\frac{1}{\sqrt{2\pi \varepsilon^2}}e^{-\frac{1}{2\varepsilon^2}(Y_i - \mu_j)^2}\Big)\\
&\propto ? 
\end{align}

Note that everything that involves the likelihood includes the products of sums, and becomes exceedingly painful. Thus, let us look at the full conditionals under data augmentation.

Data augumentation scheme 
===
- Neither the joint posterior nor any of the full conditionals involving the likelihood are of a form that's easy to sample from. 

Solution: introduce an additional set of random variables ${\{Z_i\}}_{i=1}^N$ that assign each observation to one of the mixture components with the proabilitiy of assignment being the respective mixture weight. 

If we condition on $Z_i$ we can then write the likelihood of $Y_i$ as
\begin{align*} 
p(Y_i|Z_i,\mu_1,\mu_2,\mu_3,\varepsilon^2) = \sum_{j=1}^{\textcolor{red}{3}} N(\mu_j,\varepsilon^2)\delta_{j}(Z_i) &= N(\mu_{Z_i},\varepsilon^2) \\
P(Z_i = j ) &= w_j.
\end{align*}

Data augmentation (continued)
===
- Conditional on $Z_i$ we no longer have a sum of Normal pdfs in our likelihood, resulting in a significant simplification.

- Conditional on the $\{Z_i\}$ updates will be straightforward, only depending on the mixture component that any given $Y_i$ is currently assigned to. 

- The drawback is that we also have to update ${\{Z_i\}}_{i=1}^N$ as well, introducing extra steps into our sampler. 

The updated model
===
The model is now
\begin{align*}
Y_i \mid Z_i, \mu_1, \mu_2, \mu_3, \epsilon^2 &\sim N(\mu_{Z_i}, \epsilon^2) \\
\mu_j \mid \mu_0, \sigma_0^2 &\sim N(\mu_0, \sigma_0^2) \\
Z_i \mid w_1,w_2,w_3 &\sim \text{Cat}(3, \boldsymbol{w})\\
\boldsymbol{w}= (w_1,w_2,w_3) &\sim \text{Dirichlet}(1,1,1) \\
\mu_0 &\sim N(0,3) \\
\sigma_0^2 &\sim IG(2,2) \\
\epsilon^2 &\sim IG(2,2) 
\end{align*}
$i=1,\ldots,n$
$j=1,\ldots,3$

Multinomial-Dirichlet
===
In order to proceed with the lab, we'll need to learn about the Multinomial or Categorical distribution. 

Multinomial or Categorical distribution
===

\begin{itemize}
\item $\theta = (\theta_1,\ldots,\theta_m),$
\item $X_i \in \{1,\ldots, m\},$
\item $\sum_i \theta_i =1.$
\end{itemize}

Assume that $${X} \mid {\theta} \stackrel{ind}{\sim} \text{Multinomial}({\theta})$$
or 
$$ X \mid \theta \stackrel{ind}{\sim} \text{Categorical}(\theta)$$
\vskip 1em
$$P(X_i = j \mid {\theta}) = \theta_j$$

Conjugate prior (Dirichlet)
===
$$\theta \sim \text{Dirichlet}(\alpha)$$
Recall the density of the Dirichlet is the following:
\vskip 1em

$$p(\theta \mid \alpha) \propto \prod_{j=1}^m \theta_j^{\alpha_j -1},$$ where
$\sum_j \theta_j =1, \theta_i \geq 0$ for all i

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.5\textwidth]{figures/simplex}
\end{center}

Likelihood
===

Define the data as $D = (x_1,\ldots, x_n),$ $x_i \in \{1,\ldots m\}.$
Consider
\begin{align}
p(D \mid \theta) &= \prod_{i=1}^n P(X_i = x_i \mid \theta) \\
&= \prod_{i=1}^n \theta_{x_i} \\
&=\prod_{i=1}^n \prod_{j=1}^m \theta_j^{I(x_i =j)}\\
&= \prod_{j=1}^m \theta_j^{\sum_i I(x_i = j)} \\
& = \prod_{j=1}^m \theta_j^{c_j}
\end{align}
where $c=(c_1,\ldots c_m)$, 
$c_j = \# \{i: x_i =j \}.$

Likelihood, Prior, and Posterior
===
$$p(D \mid \theta) = \prod_{j=1}^m \theta_j^{c_j}$$

\vskip 1em

$$P(\theta) \propto \prod_{j=1}^m \theta_j^{\alpha_j-1} I(\sum_j \theta_j =1, \theta_i \geq 0 \forall i)$$

Then 
\begin{align}
P(\theta \mid D) &\propto \prod_{j=1}^m \theta_j^{c_j} \times \prod_{j=1}^m \theta_j^{\alpha_j-1}  I(\sum_j \theta_j =1, \theta_i \geq 0 \forall i) \\
&\propto \prod_{j=1}^m \theta_j^{c_j + \alpha_j -1}  I(\sum_j \theta_j =1, \theta_i \geq 0 \forall i) 
\end{align}
This implies $$\theta \mid D \sim \text{Dirichlet}(c + \alpha).$$

Takeaways
===
\begin{enumerate}
\item Dirichlet is conjugate for Categorical or Multinomial.\footnote{The word Categorical seems to be used in CS and ML. The word Multinomial seems to be used in Statistics and Mathematics. I have no idea what is used in other sciences.}
\item Useful formula:
$$\prod_i \text{Multinomial} (x_i \mid \theta) \times \text{Dir}(\theta \mid \alpha) \propto \text{Dir}(\theta \mid c+ \alpha).$$
\end{enumerate}

Task 3
===
Where necessary, (re)derive the full conditionals under the data augmentation scheme.

(See the lab solutions).


Task 4
===
In task 3 you derived all the full conditionals, and due to data augmentation scheme they are all in a form that is easy to sample. Use these full conditionals to implement Gibbs sampling using the data from ``Lab8Mixture.csv''.

Task 5
===
\begin{itemize}
\item Show traceplots for all estimated parameters
\item Show means and 95\% credible intervals for the marginal posterior distributions of all the parameters
\end{itemize}
Now suppose you re-run the sampler using 3 different starting values, are your results in a,b the same? Justify your reasoning with visualizations.

Sample code
===
Partial code for this problem can be found on Sakai! 










