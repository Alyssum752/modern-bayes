
---
title: "Module 12: Linear Regression, the g-prior, and model selection"
author: "Rebecca C. Steorts"
output: 
     beamer_presentation:
      includes: 
          in_header: custom2.tex
font-size: 8px
---

Announcements
===


Agenda
===


Setup
===
- $X_{n\times p}$: regression features or covariates (design matrix)
- $x_{p \times 1}$: $i$th row vector of the regression covariates
- $y_{n\times 1}$: response variable (vector)
- $\beta_{p \times 1}$: vector of regression coefficients 

Goal: Estimation of $p(y \mid x).$

Dimensions: $y_i - \beta^T x_i = (1\times 1) - (1\times p)(p \times 1) = (1\times 1).$


Multivariate Setup 
===
Let's assume that we have data points $(x_i,y_i)$ available for all  $i=1,\ldots,n.$

- $y$ is the response variable
\[  y= \left( \begin{array}{c}
y_1\\
y_2\\
\vdots\\
y_n
\end{array} \right)_{n \times 1} \]
- $x_{i}$ is the $i$th row of the design matrix $X_{n \times p}.$

Consider the regression coefficients

\[  \beta = \left( \begin{array}{c}
\beta_{1}\\
\beta_{2}\\
\vdots\\
\beta_{p}
\end{array} \right)_{p \times 1} \]

Multivariate Setup 
===
$$y \mid X,\beta, \sigma^2 \sim MVN( X\beta, \sigma^2 I)$$
$$\beta \sim MVN(\beta_0, \Sigma_0) $$

Recall the posterior can be shown to be 
$$\bbeta \mid \bm{y}, \bX \sim MVN(\bbeta_n, \Sigma_n)$$

where

$$\bbeta_n = E[\bbeta\ \mid \bm{y}, \bX, \sigma^2] = (\Sigma_o^{-1} + (X^TX)^{-1}/\sigma^2)^{-1}
(\Sigma_o^{-1}\bbeta_0 + \bX^T\bm{y}/\sigma^2)$$

$$\Sigma_n = \text{Var}[\bbeta \mid \bm{y}, \bX, \sigma^2] = (\Sigma_o^{-1} + (X^TX)^{-1}/\sigma^2)^{-1}$$







How do we specify $\bbeta_0$ and $\Sigma_0$?

The g-prior
===
To do the \emph{least amount of calculus}, we can put a \emph{g-prior} on
$\bbeta$
$$ \bbeta \mid \bX, \bz \sim MVN(0, g\; \sigma^2 (\bX^T\bX)^{-1}).$$

$$\bbeta_n = E[\bbeta\ \mid \bm{y}, \bX, \sigma^2] 
= \frac{g}{g+1}(\Sigma_o^{-1} + (X^TX)^{-1}/\sigma^2)^{-1}
= \frac{g}{g+1} \hat{\beta}_{ols}
$$

$$\Sigma_n = \text{Var}[\bbeta \mid \bm{y}, \bX, \sigma^2] = \frac{g}{g+1}(X^TX)^{-1}/\sigma^2)^{-1}
= \frac{g}{g+1} \Var[\hat{\beta}_{ols}]$$

- g shrinks the coefficients and can prevent overfitting to the data
- if $g = n$, then as n increases, inference approximates that using $\hat{\beta}_{ols}$

Do an application and example now
===
