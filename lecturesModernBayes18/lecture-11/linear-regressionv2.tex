\documentclass[mathserif]{beamer}

\setbeamertemplate{frametitle}[default][center]%Centers the frame title.
\setbeamertemplate{navigation symbols}{}%Removes navigation symbols.
\setbeamertemplate{footline}{\raisebox{5pt}{\makebox[\paperwidth]{\hfill\makebox[10pt]{\scriptsize\insertframenumber}}}}
\setbeamertemplate{caption}[numbered]

\usepackage{amssymb,amsfonts,amsmath,latexsym,amsthm}
%\usepackage[usenames,dvipsnames]{color}
%\usepackage[]{graphicx}
%\usepackage[space]{grffile}
\usepackage{mathrsfs}   % fancy math font
% \usepackage[font=small,skip=0pt]{caption}
\usepackage[skip=0pt]{caption}
\usepackage{subcaption}
\usepackage{verbatim}
\usepackage{url}
\usepackage{bm}
\usepackage{dsfont}
\usepackage{extarrows}
\usepackage{multirow, bm}
%\input{multi_symbols.tex}


\usepackage{float,bm}
\floatstyle{boxed}
\newfloat{code}{tp}{code}
\floatname{code}{Code Example}
%\input{multi_symbols}
%\usepackage{fontspec}
%\setmainfont{Tahoma}

%\newcommand{\lam}{\lambda}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bX}   {\bm{X}}
\newcommand{\bY}   {\bm{Y}}
\newcommand{\bz}   {\bm{z}}
\newcommand{\sig}   {\Sigma}
\newcommand{\bx}{\ensuremath{\mathbf{X}}}
%\newcommand{\X}{\ensuremath{\mathbf{x}}}
%\newcommand{\w}{\ensuremath{\mathbf{w}}}
%\newcommand{\h}{\ensuremath{\mathbf{h}}}
%\newcommand{\V}{\ensuremath{\mathbf{v}}}
%\newcommand{\cov}{\text{Cov}}
\newcommand{\var}{\text{Var}}

\newcommand{\Hrule}{\rule{\linewidth}{0.2pt}}
\newcommand{\argmax}{\mathop{\mathrm{argmax}}}
\newcommand{\argmin}{\mathop{\mathrm{argmin}}}
\newcommand{\minimize}{\mathop{\mathrm{minimize}}}
\def\half{\frac{1}{2}}
\def\th{\mathrm{th}}
\def\sign{\mathrm{sign}}
\def\supp{\mathrm{supp}}
\def\E{\mathrm{E}}
\def\P{\mathrm{P}}
\def\Var{\mathrm{Var}}
\def\Cov{\mathrm{Cov}}
\def\Cor{\mathrm{Cor}}
\def\var{\mathrm{var}}
\def\cov{\mathrm{cov}}
\def\cor{\mathrm{cor}}
\def\rcor{\mathrm{rcor}}
\def\mcor{\mathrm{mcor}}
\def\mCor{\mathrm{mCor}}
\def\dCov{\mathrm{dCov}}
\def\dcov{\mathrm{dcov}}
\def\dVar{\mathrm{dVar}}
\def\dvar{\mathrm{dvar}}
\def\dCor{\mathrm{dCor}}
\def\dcor{\mathrm{dcor}}
\def\trace{\mathrm{trace}}
\def\col{\mathrm{col}}
\def\R{\mathds{R}} 
\def\cA{\mathcal{A}}
\def\cB{\mathcal{B}}
\def\cE{\mathcal{E}}
\def\cF{\mathcal{F}}
\def\cG{\mathcal{G}}
\def\cN{\mathcal{N}}
\def\hbeta{\hat{\beta}}
\def\hy{\hat{y}}
\def\red{\color[rgb]{0.8,0,0}}
\def\white{\color[rgb]{1,1,1}}
\def\blue{\color[rgb]{0,0,0.8}}
\def\green{\color[rgb]{0,0.4,0}}

%\DeclareMathOperator{\var}{Var}
%\DeclareMathOperator{\cov}{Cov}

%\newcommand{\indep}{\rotatebox{90}{\ensuremath{\models}}}
%\newcommand{\notindep}{\not\hspace{-.05in}\indep}

\usepackage{graphicx} %The mode "LaTeX => PDF" allows the following formats: .jpg  .png  .pdf  .mps
\graphicspath{{./PresentationPictures/}} %Where the figures folder is located
\usepackage{listings}
\usepackage{media9}
\usepackage{movie15}
\addmediapath{./Movies/}

\newcommand{\beginbackup}{
   \newcounter{framenumbervorappendix}
   \setcounter{framenumbervorappendix}{\value{framenumber}}
}
\newcommand{\backupend}{
   \addtocounter{framenumbervorappendix}{-\value{framenumber}}
   \addtocounter{framenumber}{\value{framenumbervorappendix}} 
}


%\usepackage{algorithm2e}
\usepackage[ruled,lined]{algorithm2e}
\def\algorithmautorefname{Algorithm}
\SetKwIF{If}{ElseIf}{Else}{if}{then}{else if}{else}{endif}
%\usepackage{times}
%\usepackage[tbtags]{amsmath}
%\usepackage{amssymb}
\usepackage{amsfonts}
%\usepackage{slfortheorems}
\usepackage{epsfig}
\usepackage{graphicx}
%\usepackage[small]{caption}
%\usepackage[square]{natbib}
%\newcommand{\newblock}{}
%\bibpunct{(}{)}{;}{a}{}{,}
%\bibliographystyle{ims}
%\usepackage[letterpaper]{geometry}
%\usepackage{color}
%\setlength{\parindent}{0pt}

\usepackage{natbib}
\bibpunct{(}{)}{;}{a}{}{,}
%\usepackage{hyperref}

\DeclareMathOperator*{\Exp}{Exp}
\DeclareMathOperator*{\TExp}{TExp}
\DeclareMathOperator*{\Bernoulli}{Bernoulli}
\DeclareMathOperator*{\Beta}{Beta}
\DeclareMathOperator*{\Ga}{Gamma}
\DeclareMathOperator*{\TGamma}{TGamma}
\DeclareMathOperator*{\Poisson}{Poisson}
\DeclareMathOperator*{\Binomial}{Binomial}
\DeclareMathOperator*{\NormalGamma}{NormalGamma}
\DeclareMathOperator*{\InvGamma}{InvGamma}
\DeclareMathOperator*{\Cauchy}{Cauchy}
\DeclareMathOperator*{\Uniform}{Uniform}
\DeclareMathOperator*{\Gumbel}{Gumbel}
\DeclareMathOperator*{\Pareto}{Pareto}
\DeclareMathOperator*{\Mono}{Mono}
\DeclareMathOperator*{\Geometric}{Geometric}
\DeclareMathOperator*{\Wishart}{Wishart}

\newcommand{\N}{\mathcal{N}}

%\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
%\newcommand{\E}{\mathbb{E}}
\renewcommand{\Pr}{\mathbb{P}}
\newcommand{\I}{\mathds{1}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\bbeta}{\mathbb{\beta}}

% Math operators
\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\median}{median}
\DeclareMathOperator*{\Vol}{Vol}

% Miscellaneous commands
\newcommand{\iid}{\stackrel{\mathrm{iid}}{\sim}}
\newcommand{\matrixsmall}[1]{\bigl(\begin{smallmatrix}#1\end{smallmatrix} \bigr)}

\newcommand{\items}[1]{\begin{itemize} #1 \end{itemize}}

\newcommand{\todo}[1]{\emph{\textcolor{red}{(#1)}}}

\newcommand{\branch}[4]{
\left\{
	\begin{array}{ll}
		#1  & \mbox{if } #2 \\
		#3 & \mbox{if } #4
	\end{array}
\right.
}

% approximately proportional to
\def\app#1#2{%
  \mathrel{%
    \setbox0=\hbox{$#1\sim$}%
    \setbox2=\hbox{%
      \rlap{\hbox{$#1\propto$}}%
      \lower1.3\ht0\box0%
    }%
    \raise0.25\ht2\box2%
  }%
}
\def\approxprop{\mathpalette\app\relax}

\newcommand{\btheta}{{\bm\theta}}
\newcommand{\bbtheta}{{\pmb{\bm\theta}}}

%\usepackage{zref-savepos}
%
%\newcounter{restofframe}
%\newsavebox{\restofframebox}
%\newlength{\mylowermargin}
%\setlength{\mylowermargin}{2pt}
%
%\newenvironment{restofframe}{%
%    \par%\centering
%    \stepcounter{restofframe}%
%    \zsavepos{restofframe-\arabic{restofframe}-begin}%
%    \begin{lrbox}{\restofframebox}%
%}{%
%    \end{lrbox}%
%    \setkeys{Gin}{keepaspectratio}%
%    \raisebox{\dimexpr-\height+\ht\strutbox\relax}[0pt][0pt]{%
%    \resizebox*{!}{\dimexpr\zposy{restofframe-\arabic{restofframe}-begin}sp-\zposy{restofframe-\arabic{restofframe}-end}sp-\mylowermargin\relax}%
%        {\usebox{\restofframebox}}%
%    }%
%    \vskip0pt plus 1filll\relax
%    \mbox{\zsavepos{restofframe-\arabic{restofframe}-end}}%
%    \par
%}


\usepackage{tikz}
\usetikzlibrary{arrows}

%\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{tkz-berge}
\usetikzlibrary{fit,shapes}

\usepackage{calc}
%%
%% The tikz package is used for doing the actual drawing.
%\usepackage{tikz}
%%
%% In order to be able to put arrowheads in the middle of directed edges, we need an extra library.
\usetikzlibrary{decorations.markings}
%%
%% The next line says how the "vertex" style of nodes should look: drawn as small circles.
\tikzstyle{vertex}=[circle, draw, inner sep=0pt, minimum size=6pt]
%%
%% Next, we make a \vertex command as a shorthand in place of \node[vertex} to get that style.
\newcommand{\vertex}{\node[vertex]}
%%
%% Finally, we declare a "counter", which is what LaTeX calls an integer variable, for use in
%% the calculations of angles for evenly spacing vertices in circular arrangements.
\newcounter{Angle}

\newtheoremstyle{example}
{\topsep} % space above
{\topsep} % space below
{} % body font
{} % indent
{\bf} % head font
{:} % punctuation between head and body
{0.5em} % space after head
{} % manually specify head
%{\thmname{#1}\thmnumber{ #2}\thmnote{:#3}} % manually specify head

\theoremstyle{example}
\newtheorem{ex}{Example}[section]

\newtheoremstyle{definition}
{\topsep} % space above
{\topsep} % space below
{} % body font
{} % indent
{\sc} % head font
{:} % punctuation between head and body
{0.5em} % space after head
{} % manually specify head
%{\thmname{#1}\thmnumber{ #2}\thmnote{:#3}} % manually specify head

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]

\theoremstyle{rem}
\newtheorem{rem}{Remark}[section]

\newtheoremstyle{theorem}
{\topsep} % space above
{\topsep} % space below
{} % body font
{} % indent
{\sc} % head font
{:} % punctuation between head and body
{0.5em} % space after head
{} % manually specify head
%{\thmname{#1}\thmnumber{ #2}\thmnote{:#3}} % manually specify head

\theoremstyle{theorm}
\newtheorem{thm}{Theorem}[section]



%%%to add in new counter for slides in beamer

%\setbeamertemplate{footline}{
%  \leavevmode%
%  \hbox{%
%  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
%    \usebeamerfont{author in head/foot}\insertshortauthor~~(\insertshortinstitute)
%  \end{beamercolorbox}%
%  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
%    \usebeamerfont{title in head/foot}\insertshorttitle
%  \end{beamercolorbox}%
%  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
%    \usebeamerfont{date in head/foot}\insertshortdate{}\hspace*{2em}
%    \insertframenumber{} \hspace*{2ex} % hier hat's sich geändert
%  \end{beamercolorbox}}%
%  \vskip0pt%
%}



%%%%%

\newcommand*\oldmacro{}
\let\oldmacro\insertshortauthor
\renewcommand*\insertshortauthor{
  \leftskip=.3cm
\insertframenumber\,/\,\inserttotalframenumber\hfill\oldmacro}




%\excludecomment{notbeamer}
%\includecomment{beamer}



\title{Linear Regression}
\author{Rebecca C. Steorts \\ Bayesian Methods and Modern Statistics: STA 360/602}
\date{Module 11 }

\begin{document}

\maketitle

\frame{
\frametitle{Setup}
Let's assume that $D_i = (x_i,y_i)$ for all $i.$
\vskip 1em 
Assume $$Y_i \stackrel{iid}{\sim} N(w^Tx_i, \sigma^2).$$
Assume $\sigma^2$ known and $\theta = w.$

\vskip 1em 
What is the MLE? 

$$\theta_{MLE} = \arg \max_{\theta \in \Theta} p(D \mid \theta)$$

}

\frame{
What is the likelihood? (Want to get to the MLE).

\vskip 1em 
Define $y = (y_1,\ldots,y_n)$. 
Note that $w^Tx_i = x_i^Tw.$
Define $A = (x_1^T,\ldots,x_n^T)$. (A is often called the design matrix). 
\begin{align}
p(D \mid \theta) &= p(y \mid x, \theta) \\
&= \prod_i p(y_i \mid x_i, \theta) \\
&= \prod_i \frac{1}{\sqrt{2 \pi \sigma^2} }
\exp\{ -1/(2\sigma^2) (y_i - w^Tx_i)^2 \}\\
&= (\frac{1}{\sqrt{2 \pi \sigma^2} })^n 
\exp\{ -1/(2\sigma^2) \sum_i (y_i - w^Tx_i)^2\} \\
&= (\frac{1}{\sqrt{2 \pi \sigma^2} })^n 
\exp\{ -1/(2\sigma^2) 
(y - Aw)^T(y-Aw)\}
\end{align}

\vskip 1em 
Goal: minimize $$(y - Aw)^T(y-Aw)$$

(Think about why we're minimizing). 
}

\frame{

Goal: minimize $$(y - Aw)^T(y-Aw)$$

\vskip 1em 

Expand what we have above. 

$$g:=(y - Aw)^T(y-Aw) = y^Ty - 2w^TA^Ty + w^TA^TAw$$


Now take the gradient or derivative with respective to w. 

$$\frac{\partial g}{\partial w} = -2A^Ty +2A^TAw =: 0.$$

This implies that 

$$A^Ty = A^TAw \implies
\hat{\theta} = (A^TA)^{-1}A^Ty$$

Why is $(A^TA)^{-1}$ invertible? (exercise). Hint: this also shows that $\hat{\theta}$ is unique! 
%Solution: $(A^TA)$ is positive definite since we assume its invertible. Thus, the gradient g is convex, so w must be a unique minimizer. 

}

\frame{
{Matrix Facts on previous slide}

Note: We're using the fact above from matrix algebra that 

$$\frac{\partial }{\partial w_j}  a^Tw = \sum_i a_iw_i = a_j.$$

The second fact we use is known as a quadratic form. Assume B is symmetric. 

\begin{align}
\frac{\partial }{\partial w_k} w^TBw &= \frac{\partial }{\partial w_k} \sum_{i,j=1}^n w_i w_j b_{ij}\\
&=
\begin{cases}
2 w_i b_{ij}, \;\text{if}\;  i=j=k \\
w_i b_{ij} \; \text{if} \;   j=k, i\neq j
\end{cases}
\end{align}

}

\frame{
We picked up some nice tricks for working with gradients. 

Also, we can identify that $\hat{\theta}$ is unbiased. (exercise). 

What is the variance of $\hat{\theta}$? (exercise). 

}



\frame{
\frametitle{Bayesian linear regression}

We derived the MLE. Why not use the MLE? 
\vskip 1em 
The MLE often overfits the data. Also, no notion of uncertainty. 

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.5\textwidth]{mle}
\label{default}
\end{center}
\end{figure}

Now suppose we want to predict a new point but what if this is the diagnostic for a patient. Or an investment for a stock portfolio. 

\vskip 1em

How certain are you? (Let's put in error bars). 





}

\frame{
\frametitle{Bayesian linear regression}



\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.5\textwidth]{mleError}
\label{default}
\end{center}
\end{figure}

Now suppose we want to predict a new point but what if this is the diagnostic for a patient. Or an investment for a stock portfolio. 

\vskip 1em

How certain are you?  We're not certain at all!



}

\frame{
Why Bayesian?

\vskip 1em

Bayesian approach allows you to say, I don't know!

\vskip 1em

We can tie back to decision theory and optimize a loss function by optimizing the predictive distribution 
$$p(y\mid x,D)$$

}

\frame{
\frametitle{Setup}

$D= (x_i, y_i)$ for all $i.$
Let $a^{-1} = 1/\sigma^2.$
\begin{align}
y_i &\mid w \stackrel{ind}{\sim} N(w^Tx_i, a^{-1})\\
w &\sim MVN(0, b^{-1}, I) \\
\end{align}
We assume that a,b are known. 
Here, $\theta =w.$

Recall: Look at the Multivariate model as these are needed to understand this module. 

}

\frame{
\frametitle{Computing the Posterior}

What is the likelihood? 
\begin{align}
p(D\mid w) \propto P(D\mid w) \propto \exp\{ -a/2 (y-Aw)^T (y-Aw)\}
\end{align}

What is the posterior? 

\begin{align}
p(w \mid D) &\propto p(D\mid w) p(w) \\
& \propto \exp\{ -a/2 (y-Aw)^T (y-Aw)\}
\times \exp\{ -b/2 w^Tw)\}
\end{align}

Just like in the Multivariate modules, we just simplify. (Check these details on your own). 

$$p(w \mid D) \propto MVN(w \mid \mu, \Lambda^{-1})
$$
where
$\Lambda = a A^TA +bI$
and $\mu = a \Lambda^{-1} A^T y.$

}

\frame{
You can show (exercise that the Maximum a Posterior estimate of $w$ is 
$$a(aA^TA +bI)^{-1}A^Ty = (A^TA + b/aI)^{-1}A^Ty$$

How does this compare to the MLE estimate? Think about this on your own! 

You will see more about Bayesian linear regression in lab. (For more on this, see Hoff). 



}
\end{document}