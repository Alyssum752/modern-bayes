\documentclass[mathserif]{beamer}

\setbeamertemplate{frametitle}[default][center]%Centers the frame title.
\setbeamertemplate{navigation symbols}{}%Removes navigation symbols.
\setbeamertemplate{footline}{\raisebox{5pt}{\makebox[\paperwidth]{\hfill\makebox[10pt]{\scriptsize\insertframenumber}}}}
\setbeamertemplate{caption}[numbered]

%\input{multi_symbols.tex}

\usepackage{float,bm}
\floatstyle{boxed}
\newfloat{code}{tp}{code}
\floatname{code}{Code Example}
\input{multi_symbols}
%\usepackage{fontspec}
%\setmainfont{Tahoma}

\newcommand{\del}   {\mbox{$\delta(x)$}}

%\newcommand{\lam}{\lambda}
%\newcommand{\bmu}{\bm{\mu}}
%%\newcommand{\bx}{\ensuremath{\mathbf{X}}}
%\newcommand{\X}{\ensuremath{\mathbf{x}}}
%\newcommand{\w}{\ensuremath{\mathbf{w}}}
%\newcommand{\h}{\ensuremath{\mathbf{h}}}
%\newcommand{\V}{\ensuremath{\mathbf{v}}}
%\newcommand{\cov}{\text{Cov}}
%\newcommand{\var{\text{Var}}}

%\DeclareMathOperator{\var}{Var}
%\DeclareMathOperator{\cov}{Cov}

%\newcommand{\indep}{\rotatebox{90}{\ensuremath{\models}}}
%\newcommand{\notindep}{\not\hspace{-.05in}\indep}

%\newcommand{\bX}   {\mbox{$\bm{X}$}}
%\newcommand{\bx}   {\mbox{$\bm{x}$}}
%\newcommand{\V}   {\mbox{\text{Var}}}
%\newcommand{\tth}   {\mbox{$\theta$}}
%\newcommand{\su}   {\mbox{$\sigma^2$}}
%\newcommand{\so}   {\mbox{$\sigma_0^2$}}
%\newcommand{\ko}   {\mbox{$\kappa_0$}}
%\newcommand{\no}   {\mbox{$\nu_0$}}
%\newcommand{\mo}   {\mbox{$\mu_0$}}
%\newcommand{\ti}   {\mbox{$\tilde{x}$}}
%\newcommand{\la}   {\mbox{$\lambda$}}

\newtheoremstyle{example}
{\topsep} % space above
{\topsep} % space below
{} % body font
{} % indent
{\bf} % head font
{:} % punctuation between head and body
{0.5em} % space after head
{} % manually specify head
%{\thmname{#1}\thmnumber{ #2}\thmnote{:#3}} % manually specify head



\newtheoremstyle{definition}
{\topsep} % space above
{\topsep} % space below
{} % body font
{} % indent
{\bf} % head font
{:} % punctuation between head and body
{0.5em} % space after head
{} % manually specify head
%{\thmname{#1}\thmnumber{ #2}\thmnote{:#3}} % manually specify head

\newtheoremstyle{algorithm}
{\topsep} % space above
{\topsep} % space below
{} % body font
{} % indent
{\bf} % head font
{:} % punctuation between head and body
{0.5em} % space after head
{} % manually specify head
%{\thmname{#1}\thmnumber{ #2}\thmnote{:#3}} % manually specify head



\newtheoremstyle{theorem}
{\topsep} % space above
{\topsep} % space below
{} % body font
{} % indent
{\bf} % head font
{:} % punctuation between head and body
{0.5em} % space after head
{} % manually specify head
%{\thmname{#1}\thmnumber{ #2}\thmnote{:#3}} % manually specify head




\usepackage{graphicx} %The mode "LaTeX => PDF" allows the following formats: .jpg  .png  .pdf  .mps
\graphicspath{{./PresentationPictures/}} %Where the figures folder is located
\usepackage{listings}
\usepackage{media9}
\usepackage{movie15}
\addmediapath{./Movies/}

\newcommand{\beginbackup}{
   \newcounter{framenumbervorappendix}
   \setcounter{framenumbervorappendix}{\value{framenumber}}
}
\newcommand{\backupend}{
   \addtocounter{framenumbervorappendix}{-\value{framenumber}}
   \addtocounter{framenumber}{\value{framenumbervorappendix}} 
}


%\usepackage{algorithm2e}
\usepackage[ruled,lined]{algorithm2e}
\def\algorithmautorefname{Algorithm}
\SetKwIF{If}{ElseIf}{Else}{if}{then}{else if}{else}{endif}
%\usepackage{times}
%\usepackage[tbtags]{amsmath}
%\usepackage{amssymb}
\usepackage{amsfonts}
%\usepackage{slfortheorems}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage[small]{caption}
%\usepackage[square]{natbib}
%\newcommand{\newblock}{}
%\bibpunct{(}{)}{;}{a}{}{,}
%\bibliographystyle{ims}
%\usepackage[letterpaper]{geometry}
\usepackage{color}
\setlength{\parindent}{0pt}

\usepackage{natbib}
\bibpunct{(}{)}{;}{a}{}{,}
%\usepackage{hyperref}



%\usepackage{zref-savepos}
%
%\newcounter{restofframe}
%\newsavebox{\restofframebox}
%\newlength{\mylowermargin}
%\setlength{\mylowermargin}{2pt}
%
%\newenvironment{restofframe}{%
%    \par%\centering
%    \stepcounter{restofframe}%
%    \zsavepos{restofframe-\arabic{restofframe}-begin}%
%    \begin{lrbox}{\restofframebox}%
%}{%
%    \end{lrbox}%
%    \setkeys{Gin}{keepaspectratio}%
%    \raisebox{\dimexpr-\height+\ht\strutbox\relax}[0pt][0pt]{%
%    \resizebox*{!}{\dimexpr\zposy{restofframe-\arabic{restofframe}-begin}sp-\zposy{restofframe-\arabic{restofframe}-end}sp-\mylowermargin\relax}%
%        {\usebox{\restofframebox}}%
%    }%
%    \vskip0pt plus 1filll\relax
%    \mbox{\zsavepos{restofframe-\arabic{restofframe}-end}}%
%    \par
%}


\usepackage{tikz}
\usetikzlibrary{arrows}

%\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{tkz-berge}
\usetikzlibrary{fit,shapes}

\usepackage{calc}
%%
%% The tikz package is used for doing the actual drawing.
%\usepackage{tikz}
%%
%% In order to be able to put arrowheads in the middle of directed edges, we need an extra library.
\usetikzlibrary{decorations.markings}
%%
%% The next line says how the "vertex" style of nodes should look: drawn as small circles.
\tikzstyle{vertex}=[circle, draw, inner sep=0pt, minimum size=6pt]
%%
%% Next, we make a \vertex command as a shorthand in place of \node[vertex} to get that style.
\newcommand{\vertex}{\node[vertex]}
%%
%% Finally, we declare a "counter", which is what LaTeX calls an integer variable, for use in
%% the calculations of angles for evenly spacing vertices in circular arrangements.
\newcounter{Angle}

\newtheoremstyle{example}
{\topsep} % space above
{\topsep} % space below
{} % body font
{} % indent
{\bf} % head font
{:} % punctuation between head and body
{0.5em} % space after head
{} % manually specify head
%{\thmname{#1}\thmnumber{ #2}\thmnote{:#3}} % manually specify head

\theoremstyle{example}
\newtheorem{ex}{Example}[section]

\newtheoremstyle{definition}
{\topsep} % space above
{\topsep} % space below
{} % body font
{} % indent
{\sc} % head font
{:} % punctuation between head and body
{0.5em} % space after head
{} % manually specify head
%{\thmname{#1}\thmnumber{ #2}\thmnote{:#3}} % manually specify head

\theoremstyle{definition}
\newtheorem{defn}{Definition}[section]

\theoremstyle{rem}
\newtheorem{rem}{Remark}[section]

\newtheoremstyle{theorem}
{\topsep} % space above
{\topsep} % space below
{} % body font
{} % indent
{\sc} % head font
{:} % punctuation between head and body
{0.5em} % space after head
{} % manually specify head
%{\thmname{#1}\thmnumber{ #2}\thmnote{:#3}} % manually specify head

\theoremstyle{theorm}
\newtheorem{thm}{Theorem}[section]



%%%to add in new counter for slides in beamer

%\setbeamertemplate{footline}{
%  \leavevmode%
%  \hbox{%
%  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{author in head/foot}%
%    \usebeamerfont{author in head/foot}\insertshortauthor~~(\insertshortinstitute)
%  \end{beamercolorbox}%
%  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
%    \usebeamerfont{title in head/foot}\insertshorttitle
%  \end{beamercolorbox}%
%  \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
%    \usebeamerfont{date in head/foot}\insertshortdate{}\hspace*{2em}
%    \insertframenumber{} \hspace*{2ex} % hier hat's sich geändert
%  \end{beamercolorbox}}%
%  \vskip0pt%
%}



%%%%%

\newcommand*\oldmacro{}
\let\oldmacro\insertshortauthor
\renewcommand*\insertshortauthor{
  \leftskip=.3cm
\insertframenumber\,/\,\inserttotalframenumber\hfill\oldmacro}




%\excludecomment{notbeamer}
%\includecomment{beamer}



\title{Noninformative (``Default") Bayes}
\author{Rebecca C. Steorts \\ Bayesian Methods and Modern Statistics: STA 360/602}

\begin{document}




\maketitle
%\frame{
%\tableofcontents
%}




%\pagestyle{plain} for plain doc
%\excludecomment{notreport}
%\includecomment{report}

%\include{cover}

%\tableofcontents
%\baselineskip 24pt
%\setlength{\parskip}{0.3cm}
%\setlength{\parindent}{0cm}
%\setcounter{chapter}{0}


%\chapter{Introduction}
%\emph{There are three kinds of lies: lies, damned lies and statistics.}\\
%---Mark Twain
%\newpage
%\frame{
%\center
%\textbf{Intro to Bayesian concepts}
%\vspace*{2em}
%
%}


\frame{
\frametitle{Agenda}
\begin{itemize}
\item Subjective prior
\item Default prior
\item Are they really noninformative?
\item Invariance property
\item Jeffreys' prior
%\item Prediction intervals
\end{itemize}
}

\frame{

\begin{itemize}
\item Ideally, we would like a \emph{subjective prior}: a prior reflecting our beliefs about the unknown parameter of interest.
\item What are some examples in practice when we have subjective information? 
\item When may we not have subjective information? 
\end{itemize}

}

\frame{
In dealing with real-life problems you may run into problems such as
\begin{itemize}
\item not having past historical data,
\item not having an expert opinion to base your prior knowledge on (perhaps your research is cutting-edge and new), or
\item as your model becomes more complicated, it becomes hard to know what priors to put on each unknown parameter.
\item What do we do in such situations?
\end{itemize}

}



%\frame{
%\begin{itemize}
%\item Suppose we could find a distribution $p(\theta)$ that contained no or little information about $\theta$ in the sense that it didn't favor one value of $\theta$ over another (provided this is possible). 
%\item Then it would be natural to refer to such a distribution as a \emph{ noninformative prior}. 
%\item We could also argue that all or most of the information contained in the posterior distribution, $p(\theta|x),$ came from the data. 
%\item Thus, all resulting inferences were \emph{objective, noninformative, default} and not subjective.
%\end{itemize}
%}

%\frame{
%
%
%\emph{Informative/subjective priors} represent our prior beliefs about parameter values before collecting any data. For example, in reality, if statisticians are unsure about specifying the prior, they will turn to the experts in the field or experimenters to look at past data to help fix the prior. 
%
%
%
%
%}
%
%\frame{
%\frametitle{Example 2.7, Carlin, Rubin, etc}
%Suppose that $X$ is the number of pregnant mothers arriving at a hospital to deliver their babies during a given month. 
%\vskip 1em
%%The discrete count nature of the data as well as its natural interpretation leads to adopting a Poisson likelihood,
%$$ p(x|\tth) = \frac{e^{-\theta}\tth^x}{x!}, \; x \in\{ 0,1,2,\ldots\}, \; \tth>0.$$
%A convenient choice for the prior distribution here is a 
%$\text{Gamma}(a,b)$.
%(Why?)
%\vskip 1em
%%since it is conjugate for the Poisson likelihood. 
%Suppose that 42 moms deliver babies during the month of December. Suppose from past data at this hospital, we assume a prior of $\text{Gamma}(5,6).$ From this, we can easily calculate the posterior distribution, posterior mean and variance, and do various calculations of interest in \texttt{R}.
%
%}


%\frame{
%
%\emph{Noninformative/objective priors} contain little or no information about $\tth$ in the sense that they do not favor one value of $\tth$ over another. Therefore, when we calculate the posterior distribution, most if not all of the inference will arise from the likelihood. Inferences in this case are \emph{objective and not subjective}.
%%Note that objective priors are often improper, yet have proper posteriors.
%Let's look at the following example to see why we might consider such priors.
%
%}
%
%\frame{
%\begin{itemize}
%\item As we noted earlier, it would be natural to take the prior on $\tth$ as $\text{Gamma}(a,b)$ since it is the conjugate prior for the Poisson likelihood. 
%\item However suppose that for this data set we do not have any information on the number of pregnant mothers arriving at the hospital so there is no basis for using a Gamma prior or any other \emph{informative} prior. 
%\item In this situation, we could take some noninformative prior.
%\end{itemize}
%}

\frame{
\frametitle{That Rule Bayes}

\centerline{\includegraphics[scale=.8]{figures/bt1}}

}

\frame{\frametitle{What did Bayes say exactly?}
\centerline{\includegraphics[scale=.8]{figures/bayespropositioninpaper}}

}

\frame{\frametitle{Translation (courtesy of Christian Robert)!}
Billiard ball $W$ rolled on a line of length one, with a uniform
probability of stopping anywhere: \\
\vspace{.1in}
$W$ stops at $p$\\
\vspace{.1in}
Second ball $O$ then rolled $n$ times under the same assumptions. \\
\vspace{.1in}
$X$ denotes the number of times the ball $O$ stopped on the left of $W$
\vspace{.1in}

Derive the posterior distribution of $p$ given $X$, when
$p \sim U[0, 1]$ and $X\mid p \sim \text{Binomial}(n, p)$

\vskip 1em

Such priors on $p$ are said to be uniform or flat. 


}

\frame{
\frametitle{Propriety of the Posterior}

Since many of the objective priors are improper, so we must check that the posterior is proper.\\

\vspace*{1em}

\begin{itemize}
\item If the prior is proper, then the posterior will \emph{always} be proper. 
\item If the prior is improper, you must check that the posterior is proper.
\end{itemize}
}




\frame{
\frametitle{A flat prior (my longer translation....)}
Let's talk about what people really mean when they use the term ``flat," since it can have different meanings.
\vskip 1em

Often statisticians will refer to a prior as being flat, when a plot of its density actually looks flat, i.e., uniform. 

$$\theta \sim \text{Unif}(0,1).$$

\vskip 1em

Why do we call it flat? It's assigning equal weight to each parameter value. Does it always do this?




}

\frame{
\begin{figure}[htdp]
\centering
\includegraphics[width = .8\textwidth]{figures/unif.pdf}
\caption{Unif(0,1) prior}
\label{fig:unif}
\end{figure}
}

\frame{

What happens if we consider though the transformation to $1/\theta.$ Is our prior still flat (does it place equal weight at every parameter value)?

\vspace*{1em}

Do this on your own using change of variables. To help answer the question, plot both the original and transformed priors. 

}

\frame{

You will find the the uniform prior is not invariant to transformations. 

\vspace*{1em}

What does invariance mean intuitively? 


}

\frame{ 
Let $\theta$ be our parameter of interest. 
\vskip 1em
Transform to $g(\theta).$
\vskip 1em
The transformation is said to be invariant (in distribution) if the distributions of 
$\theta$ and $g(\theta)$ have the same form (Normal, Beta, etc).

\vskip 1 em

Furthermore, we could have invariance of parameters:
\begin{itemize}
\item location (mean), 
\item the scale (variance), 
\item or both (mean and variance). 
\end{itemize}

\vskip 1 em


}

\frame{
\frametitle{Why  is it nice to have such a property? }

Suppose $\theta$ is the true population height in inches! 

\vskip 1 em

However, we receive some data the data is now in cm. 

\vskip 1 em

Instead of reformatting the data, we could just transform the parameter. 

\vskip 1 em

Also, we would hope that our prior is not sensitive to a slight change in our parameter (inches, cm). 


}

\frame{

One prior that is nice to work with was discovered by Jeffreys' and is invariant under one-to-one transformations of the parameter. 

}


\frame{

Suppose we consider Jeffreys' prior, $p_J(\theta),$ where $X \sim \text{Bin}(n,\theta).$ 

\vskip 1em

We calculate Jeffreys' prior by finding the Fisher information. The Fisher information tells us how much information the data gives us for certain parameter values. 

\vskip 1em
\begin{itemize}
\item Here, $p_J(\theta) \propto \text{Beta}(1/2,1/2).$ 
\item Let's consider the plot of this prior. Flat here is a purely abstract idea. 
\item In order to achieve objective inference, we need to compensate more for values on the boundary than values in the middle.
\end{itemize}


}

\frame{
\frametitle{}
\begin{figure}[htdp]
\begin{center}
\includegraphics[width = .4\textwidth]{figures/Lec7p2fig1.pdf}
\caption{Beta(1/2,1/2) and Beta(1,1) priors. }
\label{jeffreys}
\end{center}
\end{figure}

Figure~\ref{jeffreys} compares the prior density $\pi_J(\theta)$ with that for a flat prior, which is equivalent to a Beta(1,1) distribution. 

}


\frame{
\frametitle{Criticism of the Uniform Prior}

\begin{itemize}
\item The Uniform prior of Bayes (and Laplace) and has been criticized for many different reasons. 
\item We will discuss one important reason for criticism and not go into the other reasons since they go beyond the scope of this course.

\item In statistics, it is often a good property when a rule for choosing a prior is \emph{invariant} under what are called one-to-one transformations. Invariant basically means unchanging in some sense.
%The invariance principle means that if I make some one-to-one transformation of a parameter, then the rule for choosing a prior for the original parameter corresponds to the rule for choosing a prior on the transformed version of the parameter.  For example, a rule that says ``put a Uniform prior on the parameter $p$'' is not invariant: if I instead consider $\log p,$ a Uniform prior for $\log p$ does not represent the same distribution as a Uniform prior of $p.$
\item The invariance principle means that a rule for choosing a prior should provide equivalent beliefs even if we consider a transformed version of our parameter, like $p^2$ or $\log p$ instead of $p.$
\end{itemize}


}

\frame{
\frametitle{Jeffreys' Prior}

One prior that is invariant under one-to-one transformations is Jeffreys' prior. 
%(\emph{Bayesian Data Analysis}, Gelman, \emph{et al.}, p.\ 63). You are \emph{not} responsible for this invariance proof (calculus is involved).

\vskip 1em

What does the invariance principle mean? 

\vskip 1em

Suppose our prior parameter is $\theta,$ however we would like to transform to $\phi.$ 

\vskip 1em

Define $\phi = f(\theta),$ where $f$ is a one-to-one function.

\vskip 1em

Jeffreys' prior says that if $\theta$ has the distribution specified by Jeffreys' prior for $\theta,$ then $f(\theta)$ will have the distribution specified by Jeffreys' prior for $\phi.$ We will clarify by going over two examples to illustrate this idea.

}

\frame{
\frametitle{Example: Uniform}

Note, for example, that if $\theta$ has a Uniform prior, Then one can show $\phi =f(\theta)$ will not have a Uniform prior (unless $f$ is the identity function). 

%Aside from the invariance property of Jeffreys' prior, in the univariate case, Jeffreys' prior satisfies many optimality criteria that statisticians are interested in.

}

\frame{
\frametitle{Example: Jeffreys'}
Define 
$$I(\theta) = -E\left[
\frac{\partial^2 \log p(y|\theta)}{\partial \theta^2}
\right],$$ where $I(\theta)$ is called the Fisher information. Then \emph{Jeffreys' prior} is defined to be
$$p_J(\theta) = \sqrt{I(\theta)}.$$

For homework you will prove that the uniform prior in not invariant to transformation but that Jeffrey's is. 
}

\frame{
\frametitle{Example: Jeffreys'}

Suppose 
\begin{align*}
X|\theta \sim \text{Binomial}(n,\theta).
\end{align*}
Let's calculate the posterior using Jeffreys' prior. To do so we need to calculate $I(\theta).$
Ignoring terms that don't depend on $\theta,$ we find
%\begin{align*}
%\log {p(x|\theta)} &= x\log{(\theta)} + (n-x)\log{(1-\theta)} \implies\\
%\frac{\partial \log p(x|\theta)}{\partial \theta}  &= \frac{x}{\theta} - \frac{n-x}{1-\theta}\\
%\frac{\partial^2 \log p(x|\theta)}{\partial \theta^2}  &= -\frac{x}{\theta^2} - \frac{n-x}{(1-\theta)^2}\\
%\end{align*}
\vskip 10em
}

%\frame{
%\frametitle{Example: Jeffreys'}
%\vskip 10em
%Since, $E(X) = n\theta,$ then
%$$I(\theta) = -E\left[-\dfrac{x}{\theta^2} - \dfrac{n-x}{(1-\theta)^2}\right] = 
%\dfrac{n\theta}{\theta^2} +\dfrac{n-n\theta}{(1-\theta)^2}
%= \dfrac{n}{\theta}  \dfrac{n}{(1-\theta)}
%= \dfrac{n}{\theta(1-\theta)}.$$
%This implies that
%\begin{align*}
%p_J(\theta) &= \sqrt{\frac{n}{\theta(1-\theta)}}\\
%&\propto \text{Beta}(1/2,1/2).
%\end{align*}
%
%
%
%}

\frame{
\begin{figure}[htdp]
\begin{center}
\includegraphics[width = .6\textwidth]{figures/Lec7p2fig1.pdf}
\caption{Jeffreys' prior and flat prior densities}
\label{jeffreys}
\end{center}
\end{figure}
Figure~\ref{jeffreys} compares the prior density $\pi_J(\theta)$ with that for a flat prior, which is equivalent to a Beta(1,1) distribution. 
}

\frame{



%Note that in this case the prior is inversely proportional to the standard deviation. Why does this make sense?

\begin{itemize}
\item We see that the data has the least effect on the posterior when the true $\theta = 0.5,$ and has the greatest effect near 
the extremes, $\theta = 0$ or $1.$ 
\item Jeffreys' prior compensates for this by placing more mass near the extremes
of the range, where the data has the strongest effect. 
\item We could get the same effect by (for example) letting the prior be
$\pi(\theta) \propto \dfrac{1}{\text{Var}{\theta}}$ instead of $\pi(\theta) \propto \dfrac{1}{[\text{Var}{\theta}]^{1/2}}$. 
\item However, the former prior is not invariant under reparameterization, as we would prefer.
\end{itemize}
}

\frame{


We then find that
\begin{align*}
p(\theta \mid x) &\propto
\theta^x(1-\theta)^{n-x}\theta^{1/2-1}(1-\theta)^{1/2-1}\\
&=\theta^{x-1/2}(1-\theta)^{n-x-1/2}\\
&=\theta^{x-1/2+1-1}(1-\theta)^{n-x-1/2+1-1}.
\end{align*}
Thus, $\theta|x \sim \text{Beta}(x+1/2,n-x+1/2),$
which is a proper posterior since the prior is proper.\\ 
}

\frame{
\frametitle{Jeffreys' and Conjugacy}

\begin{itemize}
\item  In general, they are not conjugate priors.
\item For example, with a Gaussian model 
$X \sim N(\mu, \sigma^2),$ it can be shown that
$\pi_J(\mu) = 1$ and $\pi_J(\sigma) = \frac{1}{\sigma},$
which do not look anything like a Gaussian or an inverse gamma, respectively.
\item However, it can be shown that Jeffreys priors are limits of conjugate prior densities. 
\item For example, a Gaussian density $N(\mu_o,\sigma_o^2)$ approaches a flat prior as $\sigma_o^2 \rightarrow \infty$, while the inverse gamma 
$\sigma^{-(a+1)}e^{-b/\sigma}\to\sigma^{-1}$ as $a, b \rightarrow 0.$
\end{itemize}

}

\frame{
\frametitle{Limitations of Jeffreys'}

Jeffreys' priors work well for single-parameter models, but not for models with \textcolor{blue}{multidimensional parameters}. By analogy with the one-dimensional case, one might construct a naive Jeffreys prior as the joint density:
$$\pi_J(\theta) = |I(\theta)|^{1/2},$$
where $|\cdot|$ denotes the determinant and the $(i,j)$th element of the Fisher information matrix is given by
$$I(\theta)_{ij} = - E\left[
\frac{\partial^2 \log p(X|\theta)}{\partial \theta_i \partial \theta_j}
\right].$$
[For more reading: See PhD notes: Objective Bayes Chapter on reference priors, Gelman, et al. (2013)]
}

%\frame{
%Let's see what happens when we apply a Jeffreys' prior for $\theta$ to a multivariate Gaussian location model. Suppose 
%$$X \sim N_p(\theta,I),$$ and we are interested in performing inference on $||\theta||^2.$ 
%\begin{itemize}
%\item In this case the Jeffreys' prior for 
%$\theta$ is flat. 
%\item It turns out that the posterior has the form of a non-central $\chi^2$ distribution with $p$ degrees of freedom. 
%\item The posterior mean given one observation of $X$ is $E(||\theta||^2 \mid X) = ||X||^2 + p.$ \item This is not a good estimate because it adds $p$ to the square of the norm of $X,$ whereas we might normally want to shrink our estimate towards zero. 
%\item By contrast, the minimum variance frequentist estimate of $ ||\theta||^2$ is $||X||^2 - p.$ 
%\end{itemize}
%[To learn more, Decision theory offered this fall, Read TPE, Lehmann and Casella, 2nd Ed.]
%
%}

\frame{

What happens if we consider though the transformation to $\phi = \dfrac{1}{\theta}.$ 
\vskip 1em

\textcolor{blue}{Think we're moving from the variance to the precision!}

\begin{align}
\phi &= \frac{1}{\theta} \implies 
\theta = \frac{1}{\phi}.
\end{align}

Then using  \textcolor{blue}{\href{https://www.youtube.com/watch?v=lCKxeRiBdjQ}{a change of variables transformation}},
\begin{align}
|\frac{\partial \theta}{\partial \phi}|
& =\frac{1}{\phi^2}.
\end{align}
}

\frame{

\begin{figure}[htdp]
\centering
\includegraphics[width = .5\textwidth]{figures/unif.pdf}
\includegraphics[width = .5\textwidth]{figures/transformationPlot}
\caption{Comparison of the Uniform prior and the transformed prior on $\theta$.}
\label{fig:unif}
\end{figure}


}

\frame{
\frametitle{Recall Jeffrey's prior}

%Exercise: show that Jeffreys' prior is invariant to one-to-one transformations. 

\vskip 1 em

Let's go back over Jeffreys' when $X \sim \text{Bin}(n, \theta).$

\vskip 1 em

How do we calculate Jeffreys' prior?

\vskip 1 em

Specifically, we showed that 
$$p_J(\theta) \propto \text{Beta}(1/2, 1/2).$$

}

\frame{
\frametitle{Recall Jeffrey's prior}

%Exercise: show that Jeffreys' prior is invariant to one-to-one transformations. 



Recall $$p_J(\theta) = \sqrt{\frac{n}{\theta(1-\theta)}}.$$

What happens if we consider though the transformation to $\phi = \dfrac{1}{\theta}?$ 

%
\begin{align}
p_J(\phi) &= p_J(1/\phi) \times |\frac{\partial \theta}{\partial \phi}| \\
& = \sqrt{\frac{n}{ \frac{1}{\phi} (1- \frac{1}{\phi})}} \times \frac{1}{\phi^2}\\
& \propto \frac{\phi} {\sqrt{\phi - 1}}  \times \frac{1}{\phi^2} \\
& \propto \frac{1}{\phi \sqrt{\phi-1}},  \;\;\text{where} \; \;  1 \leq \phi < \infty.
\end{align}
Is the resulting transformation invariant? Yes! 

Can write the posterior as a Beta distribution. What is the update? 

It's invariant because $\phi = \dfrac{1}{\theta}$ is a one-to-one function.
}

%\frame{
%\frametitle{Is the transformation invariant?}
%Our transformed density $p_J(\phi)$ is \textbf{not in the form of a Beta}, meaning the distribution is not of the ``same form", hence, this transformation is not invariant. \\
%\vskip 1em
%%This should make sense. Why?
%%\vskip 1em
%%The transformation $\phi = 1/\theta$ is not a monotone transformation. (Monotone transformations and Jeffreys' priors are always invariant).
%%\vskip 1em
%%What would be an invariant transformation in this situation? 
%
%
%}

\frame{
Now consider the transformation $\theta = \frac{1}{\sqrt{\phi}}.$

\begin{align}
|\frac{\partial \theta}{\partial \phi}|
& =\frac{1}{2\phi^{3/2}}.
\end{align}

\begin{align}
p_J(1/\sqrt{\phi}) &= p_J(1/\sqrt{\phi}) \times |\frac{\partial \theta}{\partial \phi}| \\
& = (\phi)^{-1/4}
\left(\frac{
\phi^{1/2}-1
}
{\phi^{1/2}
}
\right)^{-1/2}
\times
\frac{1}{2\phi^{3/2}}\\
& \propto  \phi^{3/2} (\phi^{1/2}-1)^{-1/2}
 \;\;\text{where} \; \;  1 \leq \phi < \infty.
\end{align}

Is the resulting transformation invariant? Why or why not? 
%(It's also invariant because the transformation is a one-to-one function).
Can write the posterior as a Beta distribution. What is the update? 


%\vskip 1 em
%If you want to see a proof why this is tru
%Extra reading: \url{https://www2.stat.duke.edu/courses/Fall11/sta114/jeffreys.pdf}
%}
}

\frame{
\frametitle{Take home message with Jeffreys'}
Consider parameter $\theta$ and transformation $g(\theta).$
\vskip 1 em

Jeffreys' prior is invariant under parameterization means that Jeffreys' prior
corresponds to $g(\theta)$ is the \emph{same} as applying a change of ``measure"
or distribution to the Jeffreys' prior for $\theta. $

\vskip 1 em
Said differently, if $\theta = g(\theta),$ then 1 and 2 are the same below.  Let $J$ represent a Jeffreys' prior.

\begin{enumerate}
\item $$J_{\phi} = J_{\theta} \times |\frac{\partial g^{-1}(\theta)}{\partial \theta}| $$
\item $$J_\phi = \sqrt { I(\phi) },$$ 
\end{enumerate} 
where
$I(\phi)$ is the Fisher information of $\phi.$


}






%\frame{
%\frametitle{Up next: Dealing with complex models}
%
%
%
%
%}


\end{document}