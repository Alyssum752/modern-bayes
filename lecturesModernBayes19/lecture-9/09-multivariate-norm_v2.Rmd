
---
title: "Module 9: The Multivariate Normal Distribution"
author: "Rebecca C. Steorts"
output: 
     beamer_presentation:
      includes: 
          in_header: custom2.tex
font-size: 8px
---

Announcements
===

1. The last day of classes with be April 16, 2019
2. There will be a special lecture on April 18, 2019
by one of my PhD students on mixture models (abstract/title forthcoming). 
3. OH will be regularly scheduled until the final exam, April 29, 2019. 
4. Your lab sections will serve as extra OH by your TAs until April 29, 2019. 
5. The final exam will be April 29, 2019, 9 AM -- noon (Old Chem 116). 

Agenda
===
\begin{itemize}
\item Moving from univariate to multivariate distributions. 
\item The multivariate normal (MVN) distribution.
\item Conjugate for the MVN distribution.
\item The inverse Wishart distribution. 
\item Conjugate for the MVN distribution (but on the covariance matrix). 
\item Combining the MVN with inverse Wishart. 
\item See Chapter 7 (Hoff) for a review of the standard Normal density.
\end{itemize}

Notation
===
Assume the response variable is written as follows:

$$\bm{y}= \left( \begin{array}{c}
y_{1}\\
\textcolor{red}{y_{2}}\\
\vdots\\
y_{p}
\end{array} \right).$$

Assume that $\bm{y}_{p \times 1} \sim (\mu_{p \times 1}, \Sigma_{p \times p}),$ where

$$\bmu_{p \times 1}= \left( \begin{array}{c}
\mu_1\\
\mu_2\\
\vdots\\
\mu_p
\end{array} \right)
$$
$$
\sig_{p \times p} = 
\left( \begin{array}{cccc}
\sigma_1^2 & \sigma_{12} & \ldots&  \sigma_{1p}\\
\sigma_{21} & \sigma_2^2 & \ldots& \sigma_{2p}\\
\vdots & \vdots & \ddots & \vdots \\
\sigma_{p1} & \sigma_{p2} &\ldots& \sigma_p^2
\end{array} \right).
$$







Covaraiance matrix
===
We assume that the population mean is $\bmu_{p \times 1} = E(y)$ 

and $\Sigma_{p \times p} = \var(y) = E[(y_{p\times1} - \bmu_{p \times 1})_{p\times1}(y_{p\times1} - \bmu_{p \times 1})^T_{1\times p}],$ 
where 
\[  \bmu_{p \times 1}= \left( \begin{array}{c}
\mu_1\\
\mu_2\\
\vdots\\
\mu_p
\end{array} \right) \]
and 

$$\sig_{p \times p} = 
\left( \begin{array}{cccc}
\sigma_1^2 & \sigma_{12} & \ldots&  \sigma_{1p}\\
\sigma_{21} & \sigma_2^2 & \ldots& \sigma_{2p}\\
\vdots & \vdots & \ddots & \vdots \\
\sigma_{p1} & \sigma_{p2} &\ldots& \sigma_p^2
\end{array} \right).
$$






Notation
===
Suppose matrix $A$  is invertible. The 
$$\det(A) = \sum_{i=1}^{j=n} a_{ij} A_{ij}.$$
\vskip 1em 

I recommend using the det() commend in R. 
\vskip 1em 

Suppose now we have a square matrix $H_{p \times p}.$

$$\text{trace}(H) = \sum_i h_{ii},$$ where $h_{ii}$ are the diagonal elements of $H.$

Useful Tricks
===

Suppose that A is $n \times n$ matrix and suppose
that B is a $n \times n$ matrix. 

Lemma 1:  

$$tr(AB) = tr(BA)$$

Proof: Exercise. 

Lemma 2: 

$$x^TAx = tr(x^TAx) = tr(xx^TA) = tr(Axx^T)$$

Proof: Exercise. 


Why are these useful? We'll get to this later in this module. 


Notation
===
\begin{itemize}
\item MVN is generalization of univariate normal.
\item For the MVN, we write $\bm{y} \sim
\mathcal{MVN}(\bmu,\Sigma)$. 
\item The $(i,j)^{\text{th}}$
component of $\Sigma$ is the covariance between $X_i$ and~$X_j$ (so
the diagonal of $\Sigma$ gives the component variances).
\end{itemize}

Example: $Cov(X_1, X_2)$ is just one element of the matrix $\Sigma.$

Multivariate Normal
===
Just as the probability density of a scalar normal is
\begin{equation}
p(x) = {\left(2\pi\sigma^2\right)}^{-1/2}\exp{\left\{ -\frac{1}{2} \frac{(x-\mu)^2}{\sigma^2}\right\}},
\end{equation}
the probability density of the multivariate normal is
\begin{equation}
p(\vec{x}) = {\left(2\pi\right)}^{-p/2}(\det{\Sigma})^{-1/2} \exp{\left\{-\frac{1}{2} (\bx-\bmu)^T\Sigma^{-1} (\bx - \bmu)\right\}}.
\end{equation}
\textcolor{blue}{Univariate normal is special case of the multivariate normal with a one-dimensional mean ``vector'' and a one-by-one variance ``matrix.''}

Standard Multivariate Normal Distribution
===
Consider $$Z_1,\ldots,Z_n \stackrel{iid}{\sim} MVN(0,I)$$

\begin{align}
f_z(z) &= \prod_{i=1}^n \frac{1}{2\pi} e^{-z_i^2/2}\\
& = (2\pi)^{-n} e^{\sum_i-z_i^2/2}\\
& = (2\pi)^{-n} e^{z^Tz/2}
\end{align}

Exercise: Why does $\sum_i-z_i^2 = z^Tz$?

- E[Z] = 0
- Var[Z] = I

Conjugate to MVN
===
Suppose that $$X_1 \ldots X_n \mid \theta \stackrel{iid}{\sim} MVN(\theta, \Sigma). $$
Let $$\pi(\btheta) \sim MVN(\bmu, \Omega). $$

What is the full conditional distribution of $\btheta \mid \bX, \Sigma$?


Prior
===
\begin{align}
\pi(\btheta) &= {\left(2\pi\right)}^{-p/2}\det{\Omega}^{-1/2} \exp{\left\{-\frac{1}{2} (\btheta-\bmu)^T\Omega^{-1} (\btheta - \bmu)\right\}} \\
& \propto \exp{\left\{-\frac{1}{2} (\btheta-\bmu)^T\Omega^{-1} (\btheta - \bmu)\right\}} \\
& \propto \exp-\frac{1}{2} {\left \{\btheta^T\Omega^{-1} \btheta - 2 \btheta^T \Omega^{-1} \mu + \mu^T \Omega^{-1} \mu \right \}} \\
& \propto \exp-\frac{1}{2} {\left \{\btheta^T\Omega^{-1} \btheta - 2 \btheta^T \Omega^{-1} \mu  \right \}}\\
&= \exp-\frac{1}{2} {\left \{\btheta^TA_o \btheta - 2 \btheta^T  b_o  \right \}}
\end{align}
$\pi(\btheta) \sim MVN(\bmu, \Omega)$ implies that $A_o = \Omega^{-1}$ and $b_o = \Omega^{-1} \mu.$

Likelihood
===
\begin{align}
p(\bx \mid \btheta, \Sigma) &= \prod_{i=1}^n
{\left(2\pi\right)}^{-p/2}\det{\Sigma}^{-1/2} \exp{\left\{-\frac{1}{2} (x_i-\btheta)^T\Sigma^{-1} (x_i - \btheta)\right\}}\\
\propto 
& \exp-\frac{1}{2} {\left \{ \sum_i x_i^T \Sigma^{-1} x_i -2 \sum_i \btheta^T \Sigma^{-1} x_i + 
\sum_i \btheta^T\Sigma^{-1} \btheta 
 \right \}}\\
 & \propto \exp-\frac{1}{2} {\left \{  -2 \btheta^T \Sigma^{-1} n\bar{x} + 
n \btheta^T\Sigma^{-1} \btheta 
 \right \}}\\
  & \propto \exp-\frac{1}{2} {\left \{  -2 \btheta^T b_1+ 
\btheta^T A_1 \btheta \right \}},
\end{align}
where $$b_1= \Sigma^{-1} n\bar{x}, \quad A_1 = n\Sigma^{-1}$$ and 
$$\bar{x} := (\frac{1}{n}\sum_i x_{i1} ,\ldots, \frac{1}{n} \sum_i x_{ip})^T.$$

Full conditional
===
\begin{align}
p(\btheta \mid \bx, \Sigma) &\propto
p(\bx \mid \btheta, \Sigma) \times p(\btheta) \\
&\propto 
\exp-\frac{1}{2} {\left \{  -2 \btheta^T b_1+ 
\btheta^T A_1 \btheta \right \}} \\
&\times
\exp-\frac{1}{2} {\left \{\btheta^TA_o \btheta - 2 \btheta^T b_o  \right \}}\\
%%%
&\propto \exp\{\btheta^T b_1 - \frac{1}{2}\btheta^T A_1 \btheta- \frac{1}{2}\btheta^TA_o  \btheta
+ \btheta^T b_o\}\\
& \propto\exp\{
\btheta^T( b_o + b_1) -\frac{1}{2}\theta^T(A_o + A_1) \theta
\}
\end{align}

Full conditional
===
From the previous slide, recall that 

$$p(\btheta \mid \bx, \Sigma) \propto
\exp\{
\btheta^T( b_o + b_1) -\frac{1}{2}\theta^T(A_o + A_1) \theta
\}$$

Using the kernel of the multivariate normal, we can now find the posterior mean and the posterior covariance:

Then $$A_n = A_o + A_1 = \Omega^{-1}+n\Sigma^{-1}$$ and $$b_n = b_o + b_1 = \Omega^{-1}\mu + \Sigma^{-1} n\bar{x}$$
$$\btheta \mid \bx, \Sigma \sim MVN(A_n^{-1}b_n, A_n^{-1}) = MVN(\mu_n, \Sigma_n).$$

Interpretations
===
$$\btheta \mid \bx, \Sigma \sim MVN(A_n^{-1}b_n, A_n^{-1}) = MVN(\mu_n, \Sigma_n)$$
\begin{align}
\mu_n &= A_n^{-1}b_n = [\Omega^{-1}+n\Sigma^{-1}]^{-1}(b_o + b_1)\\
&=  [\Omega^{-1}+n\Sigma^{-1}]^{-1}(\Omega^{-1}\mu+ \Sigma^{-1} n\bar{x} )
\end{align}
\vskip 1em
\begin{align}
\Sigma_n &= A_n^{-1} = [\Omega^{-1}+n\Sigma^{-1}]^{-1}
\end{align}

inverse Wishart  distribution
===
Suppose $\Sigma \sim \text{inverseWishart}(\nu_o, S_o^{-1})$
where $\nu_o$ is a scalar and $S_o^{-1}$ is a matrix.
\vskip 1em

Then $$p(\Sigma) \propto
\det(\Sigma)^{-(\nu_o + p +1)/2} \times \exp\{
-\text{tr}(S_o\Sigma^{-1})/2
\}$$

\vskip 1em
For the full distribution, see Hoff, Chapter 7 (p. 110).

inverse Wishart distribution
===
\begin{itemize}
\item The inverse Wishart distribution is the multivariate version of the Gamma distribution. 
\item The full hierarchy we're interested in is 
$$\bm{X} \mid \btheta, \Sigma \sim MVN(\btheta, \Sigma).$$ 
$$ \btheta \sim MVN(\mu, \Omega)$$
$$ \Sigma \sim \text{inverseWishart}(\nu_o, S_o^{-1}).$$
\end{itemize}
We first consider the conjugacy of the MVN and the inverse Wishart, i.e.
$$\bm{X} \mid \btheta, \Sigma \sim MVN(\btheta, \Sigma).$$ 
$$ \Sigma \sim \text{inverseWishart}(\nu_o, S_o^{-1}).$$

Continued
===
What about $p(\Sigma \mid \bX,  \btheta) \; \textcolor{red}{\propto} \; p(\Sigma) \times p(\bX \mid \btheta, \Sigma).$
Let's first look at 
\begin{align}
&p(\bX \mid \btheta, \Sigma) \\
&\propto
\det(\Sigma)^{-n/2}\exp\{-
\sum_i (\bx_i - \btheta)^T\Sigma^{-1} (\bx_i - \btheta)/2
\}\\
&\propto
\det(\Sigma)^{-n/2}\exp\{- tr(
\sum_i  (\bx_i - \btheta)(\bx_i - \btheta)^T\Sigma^{-1}/2)
\}\\
&\propto 
\det(\Sigma)^{-n/2}\exp\{-
\text{tr}(S_\theta \Sigma^{-1}/2)
\}
\end{align}
where $S_\theta = \sum_i (\bx_i - \btheta) (\bx_i - \btheta)^T.$

Note that $$\sum_k b_k^TA b_k = tr(B B^T A),$$ where B is the matrix whose $k$th row is $b_k.$ (Here we are applying Lemma 2.)



Continued
===
Now we can calculate $p(\Sigma \mid \bX,  \btheta)$
\begin{align}
&p(\Sigma \mid \bX,  \btheta) \\ & \quad= p(\Sigma) \times p(\bX \mid \btheta, \Sigma) \\
& \quad \propto 
\det(\Sigma)^{-(\nu_o + p +1)/2} \times \exp\{
-\text{tr}(S_o\Sigma^{-1})/2
\} \\
& \qquad \times
\det(\Sigma)^{-n/2}\exp\{-
\text{tr}(S_\theta \Sigma^{-1})/2\}\\
& \quad \propto 
\det(\Sigma)^{-(\nu_o + n + p +1)/2}
\exp\{-
\text{tr}((S_o +S_\theta) \Sigma^{-1})/2\}
\end{align}
This implies that 
$$\Sigma \mid \bX,  \btheta \sim \text{inverseWishart}(\nu_o + n, [S_o + S_\theta]^{-1} =: S_n)$$

Continued
===
Suppose that we wish now to take 

$$\btheta \mid \bX, \Sigma \sim MVN(\mu_n, \Sigma_n)$$ (which we finished an example on earlier).
Now let $$\Sigma \mid \bX, \btheta \sim \text{inverseWishart}(\nu_n, S_n^{-1})$$
\vskip 1em 
There is no closed form expression for this posterior. Solution?

Gibbs sampler
===
Suppose the Gibbs sampler is at iteration $s.$
\begin{enumerate}
\item Sample $\btheta^{(s+1)}$ from it's full conditional:
\begin{enumerate}
\item[a)] Compute $\mu_n$ and $\Sigma_n$ from $\bX$ and $\Sigma^{(s)}$
\item[b)] Sample $\btheta^{(s+1)}\sim MVN(\mu_n, \Sigma_n)$
\end{enumerate}
\item Sample $\Sigma^{(s+1)}$ from its full conditional:
\begin{enumerate}
\item[a)] Compute $S_n$ from $\bX$ and $\theta^{(s+1)}$
\item[b)] Sample $\Sigma^{(s+1)} \sim \text{inverseWishart}(\nu_n, S_n^{-1})$
\end{enumerate}
\end{enumerate}


Working with Multivariate Normal Distribution
===
The `R` package, `mvtnorm`, contains functions for evaluating and simulating from a multivariate normal density.

```{r}
library(mvtnorm)
```

Simulating Data
===
Simulate a single multivariate normal random vector using the `rmvnorm` function.

```{r}
# Each row corresponds to a sample
# Here we have one sample (one row)
rmvnorm(n = 1, mean = rep(0, 2), sigma = diag(2))
```


Evaluation
===
Evaluate the multivariate normal density at a single value using the `dmvnorm` function.

```{r}
dmvnorm(rep(0, 2), mean = rep(0, 2), sigma = diag(2))
```

Working with the Multivariate Normal
===
- Now let's simulate many multivariate normals. 
- Each row is a different sample from this multivariate normal distribution.
```{r}
rmvnorm(n = 3, mean = rep(0, 2), sigma = diag(2))
```


Work with the Wishart density
===
- The `R` package, `stats`, contains functions for evaluating and simulating from a Wishart density.

- We can simulate a single Wishart distributed matrix using the `rWishart` function.

- Each row is a different sample from the Wishart distribution. 

```{r}
nu0 <- 2
Sigma0 <- diag(2)
rWishart(1, df = nu0, Sigma = Sigma0)[, , 1]
```

An Application to Reading Comprehension
===
We will follow an example from Hoff (Section 7.4, p. 112). 

A sample of 22 children are given reading comprehension tests before and after receiving a particular instructional method. 

Each student $i$ will then have two scores, $Y_{i,1}$ and $Y_{i,2}$ denoting the pre- and post-instructional scores respectively. 

Denote each student’s pair of scores $\bm{Y}_i$
$$
\bm{Y}_{i} = \left( \begin{array}{c}
Y_{i,1}\\
Y_{i,2}\\
\end{array} \right) 
= \left( \begin{array}{c}
\text{score on first test}\\
\text{score on second test}\\
\end{array} \right)
$$

Model set up
===
$$\bm{Y}_i \mid \btheta, \Sigma \sim MVN(\btheta_j, \Sigma).$$ 
$$ \btheta_j \sim MVN(\bm{\mu_0}, \Lambda_0)$$
$$ \Sigma \sim \text{inverseWishart}(\nu_o, S_o^{-1}).$$

Let $\btheta = (\theta_1, \theta_2).$

$i=1,\ldots,n$ and $j=1,2$

Prior settings
===
$$\bm{Y}_i \mid \btheta, \Sigma \sim MVN(\btheta_j, \Sigma).$$ 
$$ \btheta_j \sim MVN(\bm{\mu}_0, \Lambda_0)$$
$$ \Sigma \sim \text{inverseWishart}(\nu_o, S_o^{-1}).$$

The exam was designed to give average scores of around 50 out of 100, 
so $\bm{\mu}_0 = (50,50)^T$ would be a good choice for our prior mean.

Prior settings
===
$$\bm{Y}_i \mid \btheta, \Sigma \sim MVN(\btheta_j, \Sigma).$$ 
$$ \btheta_j \sim MVN(\bm{\mu}_0, \Lambda_0)$$
$$ \Sigma \sim \text{inverseWishart}(\nu_o, S_o^{-1}).$$

Since the true mean cannot be below 0 or above 100, we will use a prior variance that puts little probability outside of this range.

We’ll take the prior variances on $\theta_1$ and $\theta_2$ 
to be $$\lambda_{0,1}^2 = \lambda_{0,2}^2 = (50/2)^2 = 625$$ so that the prior probability that $P(\theta_j \neq [0,100]) =0.05.$

The two exams are measuring similar things, so we will take the prior correlation of 0.5 or rather $\lambda_{1,2} = 625/2 = 312.5$

Prior settings (continued)
===
$$\bm{Y}_i \mid \btheta, \Sigma \sim MVN(\btheta_j, \Sigma).$$ 
$$ \btheta_j \sim MVN(\bm{\mu}_0, \Lambda_0)$$
$$ \Sigma \sim \text{inverseWishart}(\nu_o, S_o^{-1}).$$

What about the prior settings for $\Sigma?$

We take $S_o$ to be about the same as $\Lambda_o.$

We will center $\Sigma$ around $S_o$ by setting 
$\nu_0 = p + 2 = 4.$

Load in data
===
```{r}
# read in data
Y <- structure(c(59, 43, 34, 32, 42, 38, 55, 67, 64, 
                 45, 49, 72, 34, 70, 34, 50, 41, 52, 
                 60, 34, 28, 35, 77, 39, 46, 26, 38, 
                 43, 68, 86, 77, 60, 50, 59, 38, 48, 
                 55, 58, 54, 60, 75, 47, 48, 33), 
               .Dim = c(22L, 2L), .Dimnames = list(NULL, 
                c("pretest", "posttest")))
# number of observations
```

Quick calculations
```{r}
(n <- dim(Y)[1])
(ybar <- apply(Y,2,mean))
(Sigma <- cov(Y))
```

Application to reading comprehension
===
```{r}
# set hyper-parameters
mu0 <- c(50,50)
L0 <- matrix(c(625,312.5,312.5,625),nrow=2)
nu0 <- 4
S0 <- L0
```

Gibbs sampler
===
```{r,echo=FALSE}
library (mvtnorm)
library(MCMCpack)
```

Gibbs sampler (review)
===
Suppose the Gibbs sampler is at iteration $s.$
\begin{enumerate}
\item Sample $\btheta^{(s+1)}$ from it's full conditional:
\begin{enumerate}
\item[a)] Compute $\mu_n$ and $\Sigma_n$ from $\bX$ and $\Sigma^{(s)}$
\item[b)] Sample $\btheta^{(s+1)}\sim MVN(\mu_n, \Sigma_n)$
\end{enumerate}
\item Sample $\Sigma^{(s+1)}$ from its full conditional:
\begin{enumerate}
\item[a)] Compute $S_n$ from $\bX$ and $\theta^{(s+1)}$
\item[b)] Sample $\Sigma^{(s+1)} \sim \text{inverseWishart}(\nu_n, S_n^{-1})$
\end{enumerate}
\end{enumerate}

Gibbs sampler
===
```{r}
THETA <- SIGMA <- NULL
set.seed(1)
for (s in 1:5000) {
  ## update theta
  Ln <- solve(solve(L0) + n*solve(Sigma))
  mun <- Ln %*% (solve(L0) %*% mu0 + 
                   n*solve(Sigma) %*% ybar)
  theta <- rmvnorm(1, mun, Ln)
  
  ## update Sigma
  Sn <- S0 + (t(Y) - c(theta)) %*% t(t(Y)-c(theta))
  
  Sigma <- solve(rwish(nu0 + n, solve(Sn)))
  ## save results
  THETA <- rbind(THETA, theta)
  SIGMA <- rbind(SIGMA, c(Sigma))
}
```

Posterior inference
===

Using the samples from the Gibbs sampler, we have generated 5,000 samples $$(\bm{\theta}^{(1)}, \Sigma^{(1)}, \ldots,\bm{\theta}^{(5000)}, \Sigma^{(5000)}) $$ that approxmiates $p(\bm{\theta}, \Sigma \mid y_1, \ldots, y_n).$

Glance at Gibbs sampler
===
```{r}
head(THETA)
head(SIGMA)
```

Traceplot of $\theta_1$
===
```{r, echo=FALSE}
n.iter<-5000
plot(1:n.iter, THETA[,1], pch = 16, cex = .35,
     xlab = "Iteration", ylab = expression(theta),
     main = expression(paste("Traceplot of ", theta)))
```

Traceplot of $\theta_2$
===
```{r, echo=FALSE}
plot(1:n.iter, THETA[,2], pch = 16, cex = .35,
     xlab = "Iteration", ylab = expression(theta),
     main = expression(paste("Traceplot of ", theta)))
```

Running average plot of $\theta_1$
===
```{r,echo=FALSE}
run.avg <- apply(THETA, 2, cumsum)/(1:n.iter)
plot(1:n.iter, run.avg[,1], type = "l",
     xlab = "Iteration", ylab = expression(theta),
     main = expression(paste("Running Average Plot of ", theta)))
```

Running average plot of $\theta_2$
===
```{r,echo=FALSE}
plot(1:n.iter, run.avg[,2], type = "l",
     xlab = "Iteration", ylab = expression(theta),
     main = expression(paste("Running Average Plot of ", theta)))
```

Estimated density of $\theta_1$
===
```{r, echo=FALSE}
plot(density(THETA[,1]), xlab = expression(theta), 
     main = expression(paste("Density of ", theta)))
abline(v = mean(THETA[,1]), col = "red")
```

Estimated density of $\theta_2$
===
```{r, echo=FALSE}
plot(density(THETA[,2]), xlab = expression(theta), 
     main = expression(paste("Density of ", theta)))
abline(v = mean(THETA[,2]), col = "red")
```

Traceplots and running average plots
===

The traceplots don't tell us much of anything, so this is why we examine the running average plots. Specifically, the traceplots indicate that the chain has not failed to converged. 

The running average plots indicate that the sampler appears to be mixing well by 5,000 iterations and that the chain has not failed to converged. 



Traceplots and running average plots of $\sigma$
===
Examine the trace plots and running average plots of $\Sigma$ on your own.

Return to posterior inference
===
Given our samples from our Gibbs sampler, we can approximate posterior probabilities and confidence regions. 

Confidence regions
===
```{r}
quantile(THETA[,2] - THETA[,1], prob=c(0.025,0.5,0.975))
```

Posterior inference
===
Suppose we were to give the exams/instruction to a large population, then would the average score on the second exam be higher than the first second? 

We can quanify this by calculating
$$Pr(\theta_2 > \theta_1 \mid y_1,\ldots y_n) = 0.99 
$$

```{r}
mean(THETA[,2] > THETA[,1])
```



