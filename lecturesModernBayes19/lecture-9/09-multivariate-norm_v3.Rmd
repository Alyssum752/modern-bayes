
---
title: "Module 9: The Multivariate Normal Distribution"
author: "Rebecca C. Steorts"
date: Hoff, Section 7.4
output: 
     beamer_presentation:
      includes: 
          in_header: custom2.tex
font-size: 8px
---

Announcements
===

1. The last day of classes with be April 16, 2019
2. There will be a special lecture on April 18, 2019
by one of my PhD students on mixture models (abstract/title forthcoming). 
3. OH will be regularly scheduled until the final exam, April 29, 2019. 
4. Your lab sections will serve as extra OH by your TAs until April 29, 2019. 
5. The final exam will be April 29, 2019, 9 AM -- noon (Old Chem 116). 

Agenda
===
\begin{itemize}
\item Moving from univariate to multivariate distributions. 
\item The multivariate normal (MVN) distribution.
\item Conjugate for the MVN distribution.
\item The inverse Wishart distribution. 
\item Conjugate for the MVN distribution (but on the covariance matrix). 
\item Combining the MVN with inverse Wishart. 
\item See Chapter 7 (Hoff) for a review of the standard Normal density.
\end{itemize}

Example: Reading Comprehension
===


A sample of 22 children are given reading comprehension tests before and after receiving a particular instructional method.\footnote{This example follows Hoff (Section 7.4, p. 112).}

Each student $i$ will then have two scores, $Y_{i,1}$ and $Y_{i,2}$ denoting the pre- and post-instructional scores respectively. 

Denote each studentâ€™s pair of scores by the vector $\bm{Y}_i$
$$
\bm{Y}_{i} = \left( \begin{array}{c}
Y_{i,1}\\
Y_{i,2}\\
\end{array} \right) 
= \left( \begin{array}{c}
\text{score on first test}\\
\text{score on second test}\\
\end{array} \right)
$$
where $i=1,\ldots,n$ and $p=2.$

Example: Reading Comprehension
===

What does this data look like that is observed? 

$$\bm{X}_{n \times p} = 
\left( \begin{array}{cccc}
x_{11} & x_{12} & \ldots&  x_{n1}\\
x_{21} & x_{22} & \ldots& x_{n2} \\
x_{i1} & x_{i2} & \ldots& x_{ni} \\
\vdots & \vdots & \ddots & \vdots \\
x_{n1} & x_{n2} &\ldots& x_{np}
\end{array} \right).
$$

- A row of $\bm{X}_{n \times p}$ represents a covariate we might be interested in, such as age of a person.

- Denote $x_i$ as the ith \textcolor{red}{row vector} of the $X_{n \times p}$ matrix. 

\[  x_{i}= \left( \begin{array}{c}
x_{i1}\\
\textcolor{red}{x_{i2}}\\
\vdots\\
x_{ip}
\end{array} \right) \]
where its dimension is $p \times 1.$

Example: Reading Comprehension
===

We may be interested in the population mean $\bmu_{p \times 1}.$

$$
E[\bm{Y}] =: E[\bm{Y}_{i}] = \left( \begin{array}{c}
Y_{i,1}\\
Y_{i,2}\\
\end{array} \right) 
=  \left( \begin{array}{c}
\mu_1\\
\mu_2\\
\end{array} \right) 
$$
We also may be interested in the population covariance matrix, $\Sigma.$
\begin{align}
\Sigma &= Cov(\bm{Y})
=
\left( \begin{array}{cccc}
E[Y_1^2] - E[Y_1]^2 & E[Y_1Y_2] - E[Y_1]E[Y_2] \\
E[Y_1Y_2] - E[Y_1]E[Y_2] & E[Y_2^2] - E[Y_2]^2
\end{array} \right)\\
&=
\left( \begin{array}{cccc}
\sigma_1^2 & \sigma_{1,2} \\
\sigma_{1,2} & \sigma_2^2
\end{array} \right)
\end{align}

Remark: $Cov(Y_1) = Var(Y_1) = \sigma_1^2. \qquad Cov(Y_1, Y_2) = \sigma_{1,2}.$

General Notation
===
Assume that $\bm{y}_{p \times 1} \sim (\mu_{p \times 1}, \Sigma_{p \times p}).$

$$\bm{y}_{p \times 1}= \left( \begin{array}{c}
y_{1}\\
\textcolor{red}{y_{2}}\\
\vdots\\
y_{p}
\end{array} \right).$$



$$\bmu_{p \times 1}= \left( \begin{array}{c}
\mu_1\\
\mu_2\\
\vdots\\
\mu_p
\end{array} \right)
$$
$$
\sig_{p \times p} = Cov(\bm{y}) =
\left( \begin{array}{cccc}
\sigma_1^2 & \sigma_{12} & \ldots&  \sigma_{1p}\\
\sigma_{21} & \sigma_2^2 & \ldots& \sigma_{2p}\\
\vdots & \vdots & \ddots & \vdots \\
\sigma_{p1} & \sigma_{p2} &\ldots& \sigma_p^2
\end{array} \right).
$$

Linear Algebra Background
===

Suppose matrix $A$  is invertible. The 
$$\det(A) = \sum_{i=1}^{j=n} a_{ij} A_{ij}.$$
\vskip 1em 

I recommend using the det() commend in R. 
\vskip 1em 

Suppose now we have a square matrix $H_{p \times p}.$

$$\text{trace}(H) = \sum_i h_{ii},$$ where $h_{ii}$ are the diagonal elements of $H.$

Linear Algebra Tricks
===

Suppose that A is $n \times n$ matrix and suppose
that B is a $n \times n$ matrix. 

Lemma 1:  

$$tr(AB) = tr(BA)$$
Proof: Exercise. 

Lemma 2: 

Suppose $\bm{x}$ is a vector. 

$$\bm{x}^TA\bm{x} = tr(\bm{x}^TA\bm{x}) = tr(\bm{x}\bm{x}^TA) = tr(A\bm{x}\bm{x}^T)$$

Proof: Exercise. 


Why are these useful? We'll come back to this later in the module.

Notation
===
\begin{itemize}
\item MVN is generalization of univariate normal.
\item For the MVN, we write $\bm{y} \sim
\mathcal{MVN}(\bmu,\Sigma)$. 
\item The $(i,j)^{\text{th}}$
component of $\Sigma$ is the covariance between $Y_i$ and~$Y_j$ (so
the diagonal of $\Sigma$ gives the component variances).
\end{itemize}

Example: $Cov(Y_1, Y_2)$ is just one element of the matrix $\Sigma.$

Multivariate Normal
===
Just as the probability density of a scalar normal is
\begin{equation}
p(x) = {\left(2\pi\sigma^2\right)}^{-1/2}\exp{\left\{ -\frac{1}{2} \frac{(x-\mu)^2}{\sigma^2}\right\}},
\end{equation}
the probability density of the multivariate normal is
\begin{equation}
p(\vec{x}) = {\left(2\pi\right)}^{-p/2}(\det{\Sigma})^{-1/2} \exp{\left\{-\frac{1}{2} (\bm{x}-\bmu)^T\Sigma^{-1} (\bm{x} - \bmu)\right\}}.
\end{equation}
\textcolor{blue}{Univariate normal is special case of the multivariate normal with a one-dimensional mean ``vector'' and a one-by-one variance ``matrix.''}

Standard Multivariate Normal Distribution
===
Consider $$Z_1, \ldots, Z_n \stackrel{iid}{\sim} N(0,1).$$
Show that $$Z_1,\ldots,Z_n \stackrel{iid}{\sim} MVN(0,I).$$

\begin{align}
f_z(z) &= \prod_{i=1}^n (2\pi)^{-1/2} e^{-z_i^2/2}\\
& = (2\pi)^{-n/2} e^{\sum_i-z_i^2/2}\\
& = (2\pi)^{-n/2} e^{-z^Tz/2}.
\end{align}

Exercise: Why does $\sum_i-z_i^2 = -z^Tz$?

We have just showed that $Z_1,\ldots,Z_n \stackrel{iid}{\sim} MVN(0,I).$

Conjugate to MVN
===
Suppose that $$X_1 \ldots X_n \mid \theta \stackrel{iid}{\sim} MVN(\theta, \Sigma). $$
Let $$\pi(\btheta) \sim MVN(\bmu, \Omega). $$

What is the full conditional distribution of $\btheta \mid \bm{x}, \Sigma$?


Prior
===
\begin{align}
\pi(\btheta) &= {\left(2\pi\right)}^{-p/2}\det{\Omega}^{-1/2} \exp{\left\{-\frac{1}{2} (\btheta-\bmu)^T\Omega^{-1} (\btheta - \bmu)\right\}} \\
& \propto \exp{\left\{-\frac{1}{2} (\btheta-\bmu)^T\Omega^{-1} (\btheta - \bmu)\right\}} \\
& \propto \exp-\frac{1}{2} {\left \{\btheta^T\Omega^{-1} \btheta - 2 \btheta^T \Omega^{-1} \mu + \mu^T \Omega^{-1} \mu \right \}} \\
& \propto \exp-\frac{1}{2} {\left \{\btheta^T\Omega^{-1} \btheta - 2 \btheta^T \Omega^{-1} \mu  \right \}}\\
&= \exp-\frac{1}{2} {\left \{\btheta^TA_o \btheta - 2 \btheta^T  b_o  \right \}}
\end{align}
$\pi(\btheta) \sim MVN(\bmu, \Omega)$ implies that $A_o = \Omega^{-1}$ and $b_o = \Omega^{-1} \mu.$

Likelihood
===

\begin{align}
p(\bm{x} \mid \btheta, \Sigma) &= \prod_{i=1}^n
{\left(2\pi\right)}^{-p/2}\det{\Sigma}^{-1/2} \exp{\left\{-\frac{1}{2} (x_i-\btheta)^T\Sigma^{-1} (x_i - \btheta)\right\}}\\
\propto 
& \exp-\frac{1}{2} {\left \{ \sum_i x_i^T \Sigma^{-1} x_i -2 \sum_i \btheta^T \Sigma^{-1} x_i + 
\sum_i \btheta^T\Sigma^{-1} \btheta 
 \right \}}\\
 & \propto \exp-\frac{1}{2} {\left \{  -2 \btheta^T \Sigma^{-1} n\bar{x} + 
n \btheta^T\Sigma^{-1} \btheta 
 \right \}}\\
  & \propto \exp-\frac{1}{2} {\left \{  -2 \btheta^T b_1+ 
\btheta^T A_1 \btheta \right \}},
\end{align}
where $$b_1= \Sigma^{-1} n\bar{x}, \quad A_1 = n\Sigma^{-1}$$ and 
$$\bar{x} := (\frac{1}{n}\sum_i x_{i1} ,\ldots, \frac{1}{n} \sum_i x_{ip})^T.$$


Full conditional
===

\begin{align}
p(\btheta \mid \bm{x}, \Sigma) &\propto
p(\bm{x} \mid \btheta, \Sigma) \times p(\btheta) \\
&\propto 
\exp-\frac{1}{2} {\left \{  -2 \btheta^T b_1+ 
\btheta^T A_1 \btheta \right \}} \\
&\times
\exp-\frac{1}{2} {\left \{\btheta^TA_o \btheta - 2 \btheta^T b_o  \right \}}\\
%%%
&\propto \exp\{\btheta^T b_1 - \frac{1}{2}\btheta^T A_1 \btheta- \frac{1}{2}\btheta^TA_o  \btheta
+ \btheta^T b_o\}\\
& \propto\exp\{
\btheta^T( b_o + b_1) -\frac{1}{2}\theta^T(A_o + A_1) \theta
\}
\end{align}

Full conditional
===

From the previous slide, recall that 

$$p(\btheta \mid \bm{x}, \Sigma) \propto
\exp\{
\btheta^T( b_o + b_1) -\frac{1}{2}\theta^T(A_o + A_1) \theta
\}$$

Using the kernel of the multivariate normal, we can now find the posterior mean and the posterior covariance:

Then $$A_n = A_o + A_1 = \Omega^{-1}+n\Sigma^{-1}$$ and $$b_n = b_o + b_1 = \Omega^{-1}\mu + \Sigma^{-1} n\bar{x}$$
$$\btheta \mid \bm{x}, \Sigma \sim MVN(A_n^{-1}b_n, A_n^{-1}) = MVN(\mu_n, \Sigma_n).$$

Interpretations
===

$$\btheta \mid \bm{x}, \Sigma \sim MVN(A_n^{-1}b_n, A_n^{-1}) = MVN(\mu_n, \Sigma_n)$$
\begin{align}
\mu_n &= A_n^{-1}b_n = [\Omega^{-1}+n\Sigma^{-1}]^{-1}(b_o + b_1)\\
&=  [\Omega^{-1}+n\Sigma^{-1}]^{-1}(\Omega^{-1}\mu+ \Sigma^{-1} n\bar{x} )
\end{align}
\vskip 1em
\begin{align}
\Sigma_n &= A_n^{-1} = [\Omega^{-1}+n\Sigma^{-1}]^{-1}
\end{align}

inverse Wishart  distribution
===

Suppose $\Sigma \sim \text{inverseWishart}(\nu_o, S_o^{-1})$
where $\nu_o$ is a scalar and $S_o^{-1}$ is a matrix.
\vskip 1em

Then $$p(\Sigma) \propto
\det(\Sigma)^{-(\nu_o + p +1)/2} \times \exp\{
-\text{tr}(S_o\Sigma^{-1})/2
\}$$

\vskip 1em
For the full distribution, see Hoff, Chapter 7 (p. 110).

inverse Wishart distribution
===
\begin{itemize}
\item The inverse Wishart distribution is the multivariate version of the Gamma distribution. 
\item The full hierarchy we're interested in is 
$$\bm{x} \mid \btheta, \Sigma \sim MVN(\btheta, \Sigma).$$ 
$$ \btheta \sim MVN(\mu, \Omega)$$
$$ \Sigma \sim \text{inverseWishart}(\nu_o, S_o^{-1}).$$
\end{itemize}
We first consider the conjugacy of the MVN and the inverse Wishart, i.e.
$$\bm{x} \mid \btheta, \Sigma \sim MVN(\btheta, \Sigma).$$ 
$$ \Sigma \sim \text{inverseWishart}(\nu_o, S_o^{-1}).$$

Continued
===
What about $p(\Sigma \mid \bm{x},  \btheta) \; \textcolor{red}{\propto} \; p(\Sigma) \times p(\bm{x} \mid \btheta, \Sigma).$
Let's first look at 
\begin{align}
&p(\bm{x} \mid \btheta, \Sigma) \\
&\propto
\det(\Sigma)^{-n/2}\exp\{-
\sum_i (\bm{x}_i - \btheta)^T\Sigma^{-1} (\bm{x}_i - \btheta)/2
\}\\
&\propto
\det(\Sigma)^{-n/2}\exp\{- tr(
\sum_i  (\bm{x}_i - \btheta)(\bm{x}_i - \btheta)^T\Sigma^{-1}/2)
\}\\
&\propto 
\det(\Sigma)^{-n/2}\exp\{-
\text{tr}(S_\theta \Sigma^{-1}/2)
\}
\end{align}
where $S_\theta = \sum_i (\bm{x}_i - \btheta) (\bm{x}_i - \btheta)^T.$

Note that $$\sum_k b_k^TA b_k = tr(B B^T A),$$ where B is the matrix whose $k$th row is $b_k.$ (Here we are applying Lemma 2.)

Continued
===
Now we can calculate $p(\Sigma \mid \bm{x},  \btheta)$
\begin{align}
&p(\Sigma \mid \bm{x},  \btheta) \\ & \quad= p(\Sigma) \times p(\bm{x} \mid \btheta, \Sigma) \\
& \quad \propto 
\det(\Sigma)^{-(\nu_o + p +1)/2} \times \exp\{
-\text{tr}(S_o\Sigma^{-1})/2
\} \\
& \qquad \times
\det(\Sigma)^{-n/2}\exp\{-
\text{tr}(S_\theta \Sigma^{-1})/2\}\\
& \quad \propto 
\det(\Sigma)^{-(\nu_o + n + p +1)/2}
\exp\{-
\text{tr}((S_o +S_\theta) \Sigma^{-1})/2\}
\end{align}
This implies that 
$$\Sigma \mid , \bm{x} \btheta \sim \text{inverseWishart}(\nu_o + n, [S_o + S_\theta]^{-1} =: S_n)$$

Continued
===
Suppose that we wish now to take 

$$\btheta \mid \bm{x}, \Sigma \sim MVN(\mu_n, \Sigma_n)$$ (which we finished an example on earlier).
Now let $$\Sigma \mid \bm{x}, \btheta \sim \text{inverseWishart}(\nu_n, S_n^{-1})$$
\vskip 1em 
There is no closed form expression for this posterior. Solution?

Gibbs sampler
===
Suppose the Gibbs sampler is at iteration $s.$
\begin{enumerate}
\item Sample $\btheta^{(s+1)}$ from it's full conditional:
\begin{enumerate}
\item[a)] Compute $\mu_n$ and $\Sigma_n$ from $\bX$ and $\Sigma^{(s)}$
\item[b)] Sample $\btheta^{(s+1)}\sim MVN(\mu_n, \Sigma_n)$
\end{enumerate}
\item Sample $\Sigma^{(s+1)}$ from its full conditional:
\begin{enumerate}
\item[a)] Compute $S_n$ from $\bm{x}$ and $\theta^{(s+1)}$
\item[b)] Sample $\Sigma^{(s+1)} \sim \text{inverseWishart}(\nu_n, S_n^{-1})$
\end{enumerate}
\end{enumerate}